/Users/radia78/Projects/MLM24/rotten-tomatoes-mlm24/mlm24-venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
/Users/radia78/Projects/MLM24/rotten-tomatoes-mlm24/mlm24-venv/lib/python3.9/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
EPOCH 1:
    Batch 0 | Loss: 0.9905768036842346
    Batch 1 | Loss: 0.9852988123893738
    Batch 2 | Loss: 0.9867562651634216
    Batch 3 | Loss: 0.9866766929626465
    Batch 4 | Loss: 0.9812316298484802
    Batch 5 | Loss: 0.9843094348907471
    Batch 6 | Loss: 0.9820724725723267
    Batch 7 | Loss: 0.9831937551498413
    Batch 8 | Loss: 0.9850091934204102
Train Loss: 0.9850138425827026
EPOCH 2:
    Batch 0 | Loss: 0.9866294264793396
    Batch 1 | Loss: 0.9845508933067322
    Batch 2 | Loss: 0.986281156539917
    Batch 3 | Loss: 0.98511803150177
    Batch 4 | Loss: 0.9854485392570496
    Batch 5 | Loss: 0.9862418174743652
    Batch 6 | Loss: 0.9865738153457642
    Batch 7 | Loss: 0.9879530668258667
    Batch 8 | Loss: 0.9816392660140991
Train Loss: 0.9856039881706238
EPOCH 3:
    Batch 0 | Loss: 0.985214352607727
    Batch 1 | Loss: 0.9858307838439941
    Batch 2 | Loss: 0.9830152988433838
    Batch 3 | Loss: 0.9857024550437927
    Batch 4 | Loss: 0.9842742085456848
    Batch 5 | Loss: 0.988017737865448
    Batch 6 | Loss: 0.9847173690795898
    Batch 7 | Loss: 0.9849226474761963
    Batch 8 | Loss: 0.9857768416404724
Train Loss: 0.985274612903595
EPOCH 4:
    Batch 0 | Loss: 0.9809836149215698
    Batch 1 | Loss: 0.9869654774665833
    Batch 2 | Loss: 0.9845989346504211
    Batch 3 | Loss: 0.983867347240448
    Batch 4 | Loss: 0.9866282939910889
    Batch 5 | Loss: 0.9862058758735657
    Batch 6 | Loss: 0.9889167547225952
    Batch 7 | Loss: 0.984194278717041
    Batch 8 | Loss: 0.9866083264350891
Train Loss: 0.9854409694671631
EPOCH 5:
    Batch 0 | Loss: 0.9867429137229919
    Batch 1 | Loss: 0.9837893843650818
    Batch 2 | Loss: 0.9854828715324402
    Batch 3 | Loss: 0.9881029725074768
    Batch 4 | Loss: 0.9829948544502258
    Batch 5 | Loss: 0.9876091480255127
    Batch 6 | Loss: 0.9837274551391602
    Batch 7 | Loss: 0.98518967628479
    Batch 8 | Loss: 0.9863811135292053
Train Loss: 0.9855578541755676
EPOCH 6:
    Batch 0 | Loss: 0.9861144423484802
    Batch 1 | Loss: 0.9828716516494751
    Batch 2 | Loss: 0.9827606081962585
    Batch 3 | Loss: 0.9821462631225586
    Batch 4 | Loss: 0.9837649464607239
    Batch 5 | Loss: 0.9856670498847961
    Batch 6 | Loss: 0.9860673546791077
    Batch 7 | Loss: 0.9863429069519043
    Batch 8 | Loss: 0.98813396692276
Train Loss: 0.984874427318573
EPOCH 7:
    Batch 0 | Loss: 0.9855192303657532
    Batch 1 | Loss: 0.9859746694564819
    Batch 2 | Loss: 0.9822083711624146
    Batch 3 | Loss: 0.9860888123512268
    Batch 4 | Loss: 0.986049234867096
    Batch 5 | Loss: 0.9835600256919861
    Batch 6 | Loss: 0.9865283370018005
    Batch 7 | Loss: 0.9839531779289246
    Batch 8 | Loss: 0.9811418056488037
Train Loss: 0.9845582246780396
EPOCH 8:
    Batch 0 | Loss: 0.9861594438552856
    Batch 1 | Loss: 0.988113284111023
    Batch 2 | Loss: 0.986967921257019
    Batch 3 | Loss: 0.9820766448974609
    Batch 4 | Loss: 0.9867416620254517
    Batch 5 | Loss: 0.9818477034568787
    Batch 6 | Loss: 0.9896710515022278
    Batch 7 | Loss: 0.9836373925209045
    Batch 8 | Loss: 0.9827726483345032
Train Loss: 0.9853319525718689
EPOCH 9:
    Batch 0 | Loss: 0.986070990562439
    Batch 1 | Loss: 0.9872544407844543
    Batch 2 | Loss: 0.984515368938446
    Batch 3 | Loss: 0.9819813370704651
    Batch 4 | Loss: 0.9838024973869324
    Batch 5 | Loss: 0.9812815189361572
    Batch 6 | Loss: 0.9855769276618958
    Batch 7 | Loss: 0.9830893874168396
    Batch 8 | Loss: 0.9869124293327332
Train Loss: 0.9844983220100403
EPOCH 10:
    Batch 0 | Loss: 0.9838444590568542
    Batch 1 | Loss: 0.9817805886268616
    Batch 2 | Loss: 0.9831975698471069
    Batch 3 | Loss: 0.9886034727096558
    Batch 4 | Loss: 0.9861884713172913
    Batch 5 | Loss: 0.9834060072898865
    Batch 6 | Loss: 0.9865580797195435
    Batch 7 | Loss: 0.9829463958740234
    Batch 8 | Loss: 0.9886253476142883
Train Loss: 0.9850167036056519
EPOCH 11:
    Batch 0 | Loss: 0.9839141964912415
    Batch 1 | Loss: 0.9866724610328674
    Batch 2 | Loss: 0.9866918921470642
    Batch 3 | Loss: 0.9820488691329956
    Batch 4 | Loss: 0.9887431859970093
    Batch 5 | Loss: 0.9829554557800293
    Batch 6 | Loss: 0.9860652089118958
    Batch 7 | Loss: 0.9857052564620972
    Batch 8 | Loss: 0.9914770722389221
Train Loss: 0.9860304594039917
EPOCH 12:
    Batch 0 | Loss: 0.9844059944152832
    Batch 1 | Loss: 0.9860817790031433
    Batch 2 | Loss: 0.9868017435073853
    Batch 3 | Loss: 0.9841127395629883
    Batch 4 | Loss: 0.98301100730896
    Batch 5 | Loss: 0.9852599501609802
    Batch 6 | Loss: 0.9868755340576172
    Batch 7 | Loss: 0.9870091080665588
    Batch 8 | Loss: 0.9811330437660217
Train Loss: 0.9849657416343689
EPOCH 13:
    Batch 0 | Loss: 0.9853625893592834
    Batch 1 | Loss: 0.983092725276947
    Batch 2 | Loss: 0.9885704517364502
    Batch 3 | Loss: 0.9838708639144897
    Batch 4 | Loss: 0.9851899743080139
    Batch 5 | Loss: 0.9855331778526306
    Batch 6 | Loss: 0.9836806058883667
    Batch 7 | Loss: 0.9818578362464905
    Batch 8 | Loss: 0.9813334941864014
Train Loss: 0.9842768907546997
EPOCH 14:
    Batch 0 | Loss: 0.9843252897262573
    Batch 1 | Loss: 0.9871125221252441
    Batch 2 | Loss: 0.9837360382080078
    Batch 3 | Loss: 0.9820316433906555
    Batch 4 | Loss: 0.9888495206832886
    Batch 5 | Loss: 0.9864513874053955
    Batch 6 | Loss: 0.9831655025482178
    Batch 7 | Loss: 0.9840161204338074
    Batch 8 | Loss: 0.9830793142318726
Train Loss: 0.9847519397735596
EPOCH 15:
    Batch 0 | Loss: 0.9876803755760193
    Batch 1 | Loss: 0.9847154021263123
    Batch 2 | Loss: 0.9860994815826416
    Batch 3 | Loss: 0.9837318658828735
    Batch 4 | Loss: 0.9841641187667847
    Batch 5 | Loss: 0.9882891774177551
    Batch 6 | Loss: 0.9861128926277161
    Batch 7 | Loss: 0.9861958622932434
    Batch 8 | Loss: 0.9828487634658813
Train Loss: 0.9855376482009888
EPOCH 16:
    Batch 0 | Loss: 0.9837745428085327
    Batch 1 | Loss: 0.9869111180305481
    Batch 2 | Loss: 0.9845131039619446
    Batch 3 | Loss: 0.9832427501678467
    Batch 4 | Loss: 0.9812286496162415
    Batch 5 | Loss: 0.9851952791213989
    Batch 6 | Loss: 0.9861315488815308
    Batch 7 | Loss: 0.98573899269104
    Batch 8 | Loss: 0.9868584871292114
Train Loss: 0.9848437905311584
EPOCH 17:
    Batch 0 | Loss: 0.9875867366790771
    Batch 1 | Loss: 0.98681640625
    Batch 2 | Loss: 0.9869568347930908
    Batch 3 | Loss: 0.982710063457489
    Batch 4 | Loss: 0.9824852347373962
    Batch 5 | Loss: 0.9869105815887451
    Batch 6 | Loss: 0.9848006367683411
    Batch 7 | Loss: 0.9841150045394897
    Batch 8 | Loss: 0.9861338138580322
Train Loss: 0.9853905439376831
EPOCH 18:
    Batch 0 | Loss: 0.9906994700431824
    Batch 1 | Loss: 0.9878261089324951
    Batch 2 | Loss: 0.98195880651474
    Batch 3 | Loss: 0.982927143573761
    Batch 4 | Loss: 0.9876607060432434
    Batch 5 | Loss: 0.9816216230392456
    Batch 6 | Loss: 0.9883909821510315
    Batch 7 | Loss: 0.9883331060409546
    Batch 8 | Loss: 0.982450544834137
Train Loss: 0.9857632517814636
EPOCH 19:
    Batch 0 | Loss: 0.9862048625946045
    Batch 1 | Loss: 0.9846058487892151
    Batch 2 | Loss: 0.9857558608055115
    Batch 3 | Loss: 0.9821440577507019
    Batch 4 | Loss: 0.9869322776794434
    Batch 5 | Loss: 0.9849756956100464
    Batch 6 | Loss: 0.9815053939819336
    Batch 7 | Loss: 0.9822845458984375
    Batch 8 | Loss: 0.9812302589416504
Train Loss: 0.9839598536491394
EPOCH 20:
    Batch 0 | Loss: 0.9827953577041626
    Batch 1 | Loss: 0.9878727793693542
    Batch 2 | Loss: 0.9871447086334229
    Batch 3 | Loss: 0.9840255379676819
    Batch 4 | Loss: 0.9845249652862549
    Batch 5 | Loss: 0.9860912561416626
    Batch 6 | Loss: 0.9816849231719971
    Batch 7 | Loss: 0.9889004230499268
    Batch 8 | Loss: 0.9868300557136536
Train Loss: 0.985541045665741
EPOCH 21:
    Batch 0 | Loss: 0.9827954769134521
    Batch 1 | Loss: 0.9846832156181335
    Batch 2 | Loss: 0.9834599494934082
    Batch 3 | Loss: 0.9875484704971313
    Batch 4 | Loss: 0.9847336411476135
    Batch 5 | Loss: 0.9858483672142029
    Batch 6 | Loss: 0.9881038665771484
    Batch 7 | Loss: 0.9840946793556213
    Batch 8 | Loss: 0.9881857633590698
Train Loss: 0.98549485206604
EPOCH 22:
    Batch 0 | Loss: 0.9821221828460693
    Batch 1 | Loss: 0.9827368259429932
    Batch 2 | Loss: 0.9865156412124634
    Batch 3 | Loss: 0.9830945134162903
    Batch 4 | Loss: 0.9859216809272766
    Batch 5 | Loss: 0.9854828119277954
    Batch 6 | Loss: 0.9866870641708374
    Batch 7 | Loss: 0.9841319918632507
    Batch 8 | Loss: 0.9859176874160767
Train Loss: 0.9847344160079956
EPOCH 23:
    Batch 0 | Loss: 0.9853405952453613
    Batch 1 | Loss: 0.9865561723709106
    Batch 2 | Loss: 0.9829915761947632
    Batch 3 | Loss: 0.9852838516235352
    Batch 4 | Loss: 0.985355019569397
    Batch 5 | Loss: 0.9869953393936157
    Batch 6 | Loss: 0.9873355627059937
    Batch 7 | Loss: 0.9859631061553955
    Batch 8 | Loss: 0.9862047433853149
Train Loss: 0.9857807159423828
EPOCH 24:
    Batch 0 | Loss: 0.9841964840888977
    Batch 1 | Loss: 0.9850316047668457
    Batch 2 | Loss: 0.9860100150108337
    Batch 3 | Loss: 0.9818146824836731
    Batch 4 | Loss: 0.9882045984268188
    Batch 5 | Loss: 0.9844486713409424
    Batch 6 | Loss: 0.9895045757293701
    Batch 7 | Loss: 0.9819594621658325
    Batch 8 | Loss: 0.9816343784332275
Train Loss: 0.9847560524940491
EPOCH 25:
    Batch 0 | Loss: 0.9872773885726929
    Batch 1 | Loss: 0.9834855794906616
    Batch 2 | Loss: 0.9860007166862488
    Batch 3 | Loss: 0.9855818748474121
    Batch 4 | Loss: 0.9856112599372864
    Batch 5 | Loss: 0.983305037021637
    Batch 6 | Loss: 0.9845711588859558
    Batch 7 | Loss: 0.9857359528541565
    Batch 8 | Loss: 0.9883028864860535
Train Loss: 0.9855413436889648
EPOCH 26:
    Batch 0 | Loss: 0.9854258298873901
    Batch 1 | Loss: 0.9817834496498108
    Batch 2 | Loss: 0.9844070076942444
    Batch 3 | Loss: 0.9811567068099976
    Batch 4 | Loss: 0.9880493879318237
    Batch 5 | Loss: 0.9851368069648743
    Batch 6 | Loss: 0.987899899482727
    Batch 7 | Loss: 0.9848706722259521
    Batch 8 | Loss: 0.9842574000358582
Train Loss: 0.9847763776779175
EPOCH 27:
    Batch 0 | Loss: 0.9866598844528198
    Batch 1 | Loss: 0.9838652014732361
    Batch 2 | Loss: 0.9831819534301758
    Batch 3 | Loss: 0.9864524602890015
    Batch 4 | Loss: 0.9858565330505371
    Batch 5 | Loss: 0.9860935807228088
    Batch 6 | Loss: 0.9891331195831299
    Batch 7 | Loss: 0.9868113994598389
    Batch 8 | Loss: 0.9884865283966064
Train Loss: 0.986282229423523
EPOCH 28:
    Batch 0 | Loss: 0.9847115278244019
    Batch 1 | Loss: 0.986556887626648
    Batch 2 | Loss: 0.9875140190124512
    Batch 3 | Loss: 0.9883069396018982
    Batch 4 | Loss: 0.9808711409568787
    Batch 5 | Loss: 0.9858822226524353
    Batch 6 | Loss: 0.9825901389122009
    Batch 7 | Loss: 0.9846227765083313
    Batch 8 | Loss: 0.9847694039344788
Train Loss: 0.9850917458534241
EPOCH 29:
    Batch 0 | Loss: 0.983869731426239
    Batch 1 | Loss: 0.986970841884613
    Batch 2 | Loss: 0.9851753115653992
    Batch 3 | Loss: 0.9855996370315552
    Batch 4 | Loss: 0.9845297932624817
    Batch 5 | Loss: 0.9857972264289856
    Batch 6 | Loss: 0.9864223003387451
    Batch 7 | Loss: 0.9853183031082153
    Batch 8 | Loss: 0.9839271903038025
Train Loss: 0.985289990901947
EPOCH 30:
    Batch 0 | Loss: 0.9884995222091675
    Batch 1 | Loss: 0.98421311378479
    Batch 2 | Loss: 0.9808319807052612
    Batch 3 | Loss: 0.9800023436546326
    Batch 4 | Loss: 0.9873043894767761
    Batch 5 | Loss: 0.9862445592880249
    Batch 6 | Loss: 0.9858115315437317
    Batch 7 | Loss: 0.9847739934921265
    Batch 8 | Loss: 0.982883095741272
Train Loss: 0.9845072627067566
EPOCH 31:
    Batch 0 | Loss: 0.9843721985816956
    Batch 1 | Loss: 0.9846774339675903
    Batch 2 | Loss: 0.9833342432975769
    Batch 3 | Loss: 0.9856748580932617
    Batch 4 | Loss: 0.9834803342819214
    Batch 5 | Loss: 0.9859856963157654
    Batch 6 | Loss: 0.9843701124191284
    Batch 7 | Loss: 0.9852703809738159
    Batch 8 | Loss: 0.9823030829429626
Train Loss: 0.9843853712081909
EPOCH 32:
    Batch 0 | Loss: 0.9831106066703796
    Batch 1 | Loss: 0.988517165184021
    Batch 2 | Loss: 0.9837754964828491
    Batch 3 | Loss: 0.9863322377204895
    Batch 4 | Loss: 0.9889389276504517
    Batch 5 | Loss: 0.9861487150192261
    Batch 6 | Loss: 0.9828386306762695
    Batch 7 | Loss: 0.9838597178459167
    Batch 8 | Loss: 0.9907028675079346
Train Loss: 0.9860249757766724
EPOCH 33:
    Batch 0 | Loss: 0.9824607968330383
    Batch 1 | Loss: 0.9858846068382263
    Batch 2 | Loss: 0.9859672784805298
    Batch 3 | Loss: 0.9822190999984741
    Batch 4 | Loss: 0.9866021275520325
    Batch 5 | Loss: 0.9863982796669006
    Batch 6 | Loss: 0.9853734970092773
    Batch 7 | Loss: 0.9854018688201904
    Batch 8 | Loss: 0.9864628911018372
Train Loss: 0.9851967692375183
EPOCH 34:
    Batch 0 | Loss: 0.9845449924468994
    Batch 1 | Loss: 0.9866302013397217
    Batch 2 | Loss: 0.9868422150611877
    Batch 3 | Loss: 0.9848089814186096
    Batch 4 | Loss: 0.9870851039886475
    Batch 5 | Loss: 0.9836212992668152
    Batch 6 | Loss: 0.984510600566864
    Batch 7 | Loss: 0.9880024194717407
    Batch 8 | Loss: 0.9857280850410461
Train Loss: 0.9857526421546936
EPOCH 35:
    Batch 0 | Loss: 0.9867665767669678
    Batch 1 | Loss: 0.9834866523742676
    Batch 2 | Loss: 0.9841923713684082
    Batch 3 | Loss: 0.986494243144989
    Batch 4 | Loss: 0.9846801161766052
    Batch 5 | Loss: 0.9881244897842407
    Batch 6 | Loss: 0.9864575862884521
    Batch 7 | Loss: 0.9842720627784729
    Batch 8 | Loss: 0.9893955588340759
Train Loss: 0.9859854578971863
EPOCH 36:
    Batch 0 | Loss: 0.9827659130096436
    Batch 1 | Loss: 0.9842798709869385
    Batch 2 | Loss: 0.9859940409660339
    Batch 3 | Loss: 0.9865257143974304
    Batch 4 | Loss: 0.9836001992225647
    Batch 5 | Loss: 0.9837893843650818
    Batch 6 | Loss: 0.9847432971000671
    Batch 7 | Loss: 0.9829091429710388
    Batch 8 | Loss: 0.9895746111869812
Train Loss: 0.984909176826477
EPOCH 37:
    Batch 0 | Loss: 0.9845525026321411
    Batch 1 | Loss: 0.9850466251373291
    Batch 2 | Loss: 0.9867537617683411
    Batch 3 | Loss: 0.9843789339065552
    Batch 4 | Loss: 0.9859418869018555
    Batch 5 | Loss: 0.9856605529785156
    Batch 6 | Loss: 0.9878332018852234
    Batch 7 | Loss: 0.9876178503036499
    Batch 8 | Loss: 0.9840664267539978
Train Loss: 0.9857613444328308
EPOCH 38:
    Batch 0 | Loss: 0.9819912910461426
    Batch 1 | Loss: 0.9880709648132324
    Batch 2 | Loss: 0.9823644161224365
    Batch 3 | Loss: 0.9853852391242981
    Batch 4 | Loss: 0.9875950217247009
    Batch 5 | Loss: 0.9845666289329529
    Batch 6 | Loss: 0.9874796271324158
    Batch 7 | Loss: 0.9823055267333984
    Batch 8 | Loss: 0.9883304238319397
Train Loss: 0.9853432774543762
EPOCH 39:
    Batch 0 | Loss: 0.9816563725471497
    Batch 1 | Loss: 0.9855324029922485
    Batch 2 | Loss: 0.9835376739501953
    Batch 3 | Loss: 0.9851707220077515
    Batch 4 | Loss: 0.9901737570762634
    Batch 5 | Loss: 0.982167661190033
    Batch 6 | Loss: 0.9819692969322205
    Batch 7 | Loss: 0.9862015247344971
    Batch 8 | Loss: 0.9806767702102661
Train Loss: 0.9841206669807434
EPOCH 40:
    Batch 0 | Loss: 0.9850402474403381
    Batch 1 | Loss: 0.9881434440612793
    Batch 2 | Loss: 0.9856627583503723
    Batch 3 | Loss: 0.9855055212974548
    Batch 4 | Loss: 0.9810355305671692
    Batch 5 | Loss: 0.9870566725730896
    Batch 6 | Loss: 0.9819920063018799
    Batch 7 | Loss: 0.9861571192741394
    Batch 8 | Loss: 0.9870177507400513
Train Loss: 0.9852901101112366
EPOCH 41:
    Batch 0 | Loss: 0.986237645149231
    Batch 1 | Loss: 0.9859054088592529
    Batch 2 | Loss: 0.985288143157959
    Batch 3 | Loss: 0.9885029792785645
    Batch 4 | Loss: 0.9886997938156128
    Batch 5 | Loss: 0.9882598519325256
    Batch 6 | Loss: 0.9864996075630188
    Batch 7 | Loss: 0.9839360117912292
    Batch 8 | Loss: 0.9840987324714661
Train Loss: 0.9863808751106262
EPOCH 42:
    Batch 0 | Loss: 0.9849131107330322
    Batch 1 | Loss: 0.983311653137207
    Batch 2 | Loss: 0.9855014085769653
    Batch 3 | Loss: 0.9853845834732056
    Batch 4 | Loss: 0.9844825267791748
    Batch 5 | Loss: 0.9834846258163452
    Batch 6 | Loss: 0.98691326379776
    Batch 7 | Loss: 0.9866107106208801
    Batch 8 | Loss: 0.9845446348190308
Train Loss: 0.9850162863731384
EPOCH 43:
    Batch 0 | Loss: 0.9848092794418335
    Batch 1 | Loss: 0.9857099056243896
    Batch 2 | Loss: 0.9842876195907593
    Batch 3 | Loss: 0.9883756041526794
    Batch 4 | Loss: 0.9876841306686401
    Batch 5 | Loss: 0.9827824234962463
    Batch 6 | Loss: 0.9810557961463928
    Batch 7 | Loss: 0.9855540990829468
    Batch 8 | Loss: 0.9810168147087097
Train Loss: 0.9845861792564392
EPOCH 44:
    Batch 0 | Loss: 0.9848770499229431
    Batch 1 | Loss: 0.9834720492362976
    Batch 2 | Loss: 0.9872774481773376
    Batch 3 | Loss: 0.9835578203201294
    Batch 4 | Loss: 0.9864699244499207
    Batch 5 | Loss: 0.9869385361671448
    Batch 6 | Loss: 0.9862570762634277
    Batch 7 | Loss: 0.9864526987075806
    Batch 8 | Loss: 0.9858814477920532
Train Loss: 0.9856870174407959
EPOCH 45:
    Batch 0 | Loss: 0.9863576292991638
    Batch 1 | Loss: 0.9850561022758484
    Batch 2 | Loss: 0.9842378497123718
    Batch 3 | Loss: 0.9882847666740417
    Batch 4 | Loss: 0.985054075717926
    Batch 5 | Loss: 0.9842864871025085
    Batch 6 | Loss: 0.9822253584861755
    Batch 7 | Loss: 0.9854172468185425
    Batch 8 | Loss: 0.9860658049583435
Train Loss: 0.9852206110954285
EPOCH 46:
    Batch 0 | Loss: 0.9849539995193481
    Batch 1 | Loss: 0.9870679378509521
    Batch 2 | Loss: 0.9869706630706787
    Batch 3 | Loss: 0.982886016368866
    Batch 4 | Loss: 0.9823588728904724
    Batch 5 | Loss: 0.9858057498931885
    Batch 6 | Loss: 0.9834693670272827
    Batch 7 | Loss: 0.985668957233429
    Batch 8 | Loss: 0.982386589050293
Train Loss: 0.9846187233924866
EPOCH 47:
    Batch 0 | Loss: 0.9872725605964661
    Batch 1 | Loss: 0.9850284457206726
    Batch 2 | Loss: 0.9819377660751343
    Batch 3 | Loss: 0.9887761473655701
    Batch 4 | Loss: 0.9848823547363281
    Batch 5 | Loss: 0.9864552021026611
    Batch 6 | Loss: 0.9820008873939514
    Batch 7 | Loss: 0.9839386940002441
    Batch 8 | Loss: 0.9870585203170776
Train Loss: 0.9852612018585205
EPOCH 48:
    Batch 0 | Loss: 0.9881104826927185
    Batch 1 | Loss: 0.9863139390945435
    Batch 2 | Loss: 0.9862223267555237
    Batch 3 | Loss: 0.9865366220474243
    Batch 4 | Loss: 0.9848465323448181
    Batch 5 | Loss: 0.9821760058403015
    Batch 6 | Loss: 0.9821937680244446
    Batch 7 | Loss: 0.9865755438804626
    Batch 8 | Loss: 0.9826567769050598
Train Loss: 0.9850702285766602
EPOCH 49:
    Batch 0 | Loss: 0.9831196069717407
    Batch 1 | Loss: 0.9839878082275391
    Batch 2 | Loss: 0.988128662109375
    Batch 3 | Loss: 0.9889070987701416
    Batch 4 | Loss: 0.983769416809082
    Batch 5 | Loss: 0.9848838448524475
    Batch 6 | Loss: 0.9834659695625305
    Batch 7 | Loss: 0.9842028617858887
    Batch 8 | Loss: 0.9876120686531067
Train Loss: 0.9853419065475464
EPOCH 50:
    Batch 0 | Loss: 0.9858289361000061
    Batch 1 | Loss: 0.9855818748474121
    Batch 2 | Loss: 0.9849948883056641
    Batch 3 | Loss: 0.9864596128463745
    Batch 4 | Loss: 0.9912147521972656
    Batch 5 | Loss: 0.9852872490882874
    Batch 6 | Loss: 0.9807632565498352
    Batch 7 | Loss: 0.9859068989753723
    Batch 8 | Loss: 0.9888808727264404
Train Loss: 0.9861021041870117
EPOCH 51:
    Batch 0 | Loss: 0.9859994053840637
    Batch 1 | Loss: 0.9880996942520142
    Batch 2 | Loss: 0.9817037582397461
    Batch 3 | Loss: 0.9835482835769653
    Batch 4 | Loss: 0.9886090755462646
    Batch 5 | Loss: 0.9829589128494263
    Batch 6 | Loss: 0.9879502654075623
    Batch 7 | Loss: 0.983883798122406
    Batch 8 | Loss: 0.9907107353210449
Train Loss: 0.9859405159950256
EPOCH 52:
    Batch 0 | Loss: 0.9835881590843201
    Batch 1 | Loss: 0.9895467162132263
    Batch 2 | Loss: 0.9851431250572205
    Batch 3 | Loss: 0.986520528793335
    Batch 4 | Loss: 0.9854622483253479
    Batch 5 | Loss: 0.9849230647087097
    Batch 6 | Loss: 0.9858166575431824
    Batch 7 | Loss: 0.9847618937492371
    Batch 8 | Loss: 0.980895459651947
Train Loss: 0.9851841330528259
EPOCH 53:
    Batch 0 | Loss: 0.9863001704216003
    Batch 1 | Loss: 0.983448326587677
    Batch 2 | Loss: 0.9842796921730042
    Batch 3 | Loss: 0.9860098958015442
    Batch 4 | Loss: 0.9860174059867859
    Batch 5 | Loss: 0.9845309257507324
    Batch 6 | Loss: 0.9882123470306396
    Batch 7 | Loss: 0.9821193218231201
    Batch 8 | Loss: 0.9863643646240234
Train Loss: 0.985253632068634
EPOCH 54:
    Batch 0 | Loss: 0.9861070513725281
    Batch 1 | Loss: 0.984329104423523
    Batch 2 | Loss: 0.9873290061950684
    Batch 3 | Loss: 0.9836251735687256
    Batch 4 | Loss: 0.9853833913803101
    Batch 5 | Loss: 0.9885271191596985
    Batch 6 | Loss: 0.9849205017089844
    Batch 7 | Loss: 0.9855424761772156
    Batch 8 | Loss: 0.986467719078064
Train Loss: 0.985803484916687
EPOCH 55:
    Batch 0 | Loss: 0.9852866530418396
    Batch 1 | Loss: 0.9883802533149719
    Batch 2 | Loss: 0.9872316718101501
    Batch 3 | Loss: 0.9840720891952515
    Batch 4 | Loss: 0.9847557544708252
    Batch 5 | Loss: 0.9865682721138
    Batch 6 | Loss: 0.9850238561630249
    Batch 7 | Loss: 0.983396053314209
    Batch 8 | Loss: 0.9870637059211731
Train Loss: 0.9857531785964966
EPOCH 56:
    Batch 0 | Loss: 0.9869954586029053
    Batch 1 | Loss: 0.9803226590156555
    Batch 2 | Loss: 0.9898960590362549
    Batch 3 | Loss: 0.9873159527778625
    Batch 4 | Loss: 0.9833640456199646
    Batch 5 | Loss: 0.9836075901985168
    Batch 6 | Loss: 0.9822219014167786
    Batch 7 | Loss: 0.9878295660018921
    Batch 8 | Loss: 0.9856541156768799
Train Loss: 0.9852452874183655
EPOCH 57:
    Batch 0 | Loss: 0.9872332811355591
    Batch 1 | Loss: 0.9847669005393982
    Batch 2 | Loss: 0.9831535220146179
    Batch 3 | Loss: 0.9892330765724182
    Batch 4 | Loss: 0.9884993433952332
    Batch 5 | Loss: 0.9845994114875793
    Batch 6 | Loss: 0.9826028943061829
    Batch 7 | Loss: 0.9826061725616455
    Batch 8 | Loss: 0.9834221005439758
Train Loss: 0.9851241707801819
EPOCH 58:
    Batch 0 | Loss: 0.9849436283111572
    Batch 1 | Loss: 0.9868322610855103
    Batch 2 | Loss: 0.9824338555335999
    Batch 3 | Loss: 0.9874241948127747
    Batch 4 | Loss: 0.9817497730255127
    Batch 5 | Loss: 0.9832376837730408
    Batch 6 | Loss: 0.9816691875457764
    Batch 7 | Loss: 0.983201801776886
    Batch 8 | Loss: 0.9865437746047974
Train Loss: 0.9842262268066406
EPOCH 59:
    Batch 0 | Loss: 0.9871506690979004
    Batch 1 | Loss: 0.9834059476852417
    Batch 2 | Loss: 0.9853283762931824
    Batch 3 | Loss: 0.9841479063034058
    Batch 4 | Loss: 0.9829242825508118
    Batch 5 | Loss: 0.9847914576530457
    Batch 6 | Loss: 0.9808740019798279
    Batch 7 | Loss: 0.9838259816169739
    Batch 8 | Loss: 0.9898802638053894
Train Loss: 0.9847033023834229
EPOCH 60:
    Batch 0 | Loss: 0.9815449118614197
    Batch 1 | Loss: 0.984008252620697
    Batch 2 | Loss: 0.9841712713241577
    Batch 3 | Loss: 0.9839425683021545
    Batch 4 | Loss: 0.9857551455497742
    Batch 5 | Loss: 0.9836534857749939
    Batch 6 | Loss: 0.9871517419815063
    Batch 7 | Loss: 0.983741283416748
    Batch 8 | Loss: 0.9857710599899292
Train Loss: 0.9844154715538025
EPOCH 61:
    Batch 0 | Loss: 0.9866493940353394
    Batch 1 | Loss: 0.9820179343223572
    Batch 2 | Loss: 0.9822785258293152
    Batch 3 | Loss: 0.9851338267326355
    Batch 4 | Loss: 0.9877269268035889
    Batch 5 | Loss: 0.9835649728775024
    Batch 6 | Loss: 0.9854885339736938
    Batch 7 | Loss: 0.9876702427864075
    Batch 8 | Loss: 0.9851458668708801
Train Loss: 0.9850751161575317
EPOCH 62:
    Batch 0 | Loss: 0.9812341928482056
    Batch 1 | Loss: 0.9853404760360718
    Batch 2 | Loss: 0.9853885769844055
    Batch 3 | Loss: 0.9826552867889404
    Batch 4 | Loss: 0.9816821813583374
    Batch 5 | Loss: 0.9837010502815247
    Batch 6 | Loss: 0.9871395826339722
    Batch 7 | Loss: 0.9873747825622559
    Batch 8 | Loss: 0.9870609045028687
Train Loss: 0.984619677066803
EPOCH 63:
    Batch 0 | Loss: 0.986535906791687
    Batch 1 | Loss: 0.9846141934394836
    Batch 2 | Loss: 0.9853581786155701
    Batch 3 | Loss: 0.984576940536499
    Batch 4 | Loss: 0.9841759204864502
    Batch 5 | Loss: 0.985292911529541
    Batch 6 | Loss: 0.9880481362342834
    Batch 7 | Loss: 0.9841873049736023
    Batch 8 | Loss: 0.9866253733634949
Train Loss: 0.9854905605316162
EPOCH 64:
    Batch 0 | Loss: 0.9831996560096741
    Batch 1 | Loss: 0.9832568168640137
    Batch 2 | Loss: 0.9838377833366394
    Batch 3 | Loss: 0.9850331544876099
    Batch 4 | Loss: 0.9882922172546387
    Batch 5 | Loss: 0.9824987053871155
    Batch 6 | Loss: 0.987118124961853
    Batch 7 | Loss: 0.9872881770133972
    Batch 8 | Loss: 0.9846662282943726
Train Loss: 0.98502117395401
EPOCH 65:
    Batch 0 | Loss: 0.9869613647460938
    Batch 1 | Loss: 0.9844294190406799
    Batch 2 | Loss: 0.9855226278305054
    Batch 3 | Loss: 0.9897215366363525
    Batch 4 | Loss: 0.9866676926612854
    Batch 5 | Loss: 0.9843258261680603
    Batch 6 | Loss: 0.9877209067344666
    Batch 7 | Loss: 0.9844511151313782
    Batch 8 | Loss: 0.9784923195838928
Train Loss: 0.9853659868240356
EPOCH 66:
    Batch 0 | Loss: 0.9818007349967957
    Batch 1 | Loss: 0.9844956398010254
    Batch 2 | Loss: 0.9875103831291199
    Batch 3 | Loss: 0.9836195111274719
    Batch 4 | Loss: 0.9872484803199768
    Batch 5 | Loss: 0.9833605289459229
    Batch 6 | Loss: 0.983380138874054
    Batch 7 | Loss: 0.9875940680503845
    Batch 8 | Loss: 0.9863234162330627
Train Loss: 0.985037088394165
EPOCH 67:
    Batch 0 | Loss: 0.9831947684288025
    Batch 1 | Loss: 0.9817299842834473
    Batch 2 | Loss: 0.9830815196037292
    Batch 3 | Loss: 0.9844966530799866
    Batch 4 | Loss: 0.987838864326477
    Batch 5 | Loss: 0.9850755333900452
    Batch 6 | Loss: 0.9822437763214111
    Batch 7 | Loss: 0.9842251539230347
    Batch 8 | Loss: 0.9830368161201477
Train Loss: 0.983880341053009
EPOCH 68:
    Batch 0 | Loss: 0.9841359257698059
    Batch 1 | Loss: 0.9842106699943542
    Batch 2 | Loss: 0.9814561605453491
    Batch 3 | Loss: 0.9822078943252563
    Batch 4 | Loss: 0.9841497540473938
    Batch 5 | Loss: 0.9876653552055359
    Batch 6 | Loss: 0.9819129705429077
    Batch 7 | Loss: 0.9871222972869873
    Batch 8 | Loss: 0.9833206534385681
Train Loss: 0.9840201139450073
EPOCH 69:
    Batch 0 | Loss: 0.9821859002113342
    Batch 1 | Loss: 0.9860679507255554
    Batch 2 | Loss: 0.9875050187110901
    Batch 3 | Loss: 0.983400285243988
    Batch 4 | Loss: 0.9833831191062927
    Batch 5 | Loss: 0.9844829440116882
    Batch 6 | Loss: 0.9812816977500916
    Batch 7 | Loss: 0.9839250445365906
    Batch 8 | Loss: 0.9858801364898682
Train Loss: 0.9842345714569092
EPOCH 70:
    Batch 0 | Loss: 0.9843176603317261
    Batch 1 | Loss: 0.9858478903770447
    Batch 2 | Loss: 0.9869699478149414
    Batch 3 | Loss: 0.9855680465698242
    Batch 4 | Loss: 0.9826510548591614
    Batch 5 | Loss: 0.9840558171272278
    Batch 6 | Loss: 0.9829294681549072
    Batch 7 | Loss: 0.9857475161552429
    Batch 8 | Loss: 0.9862852692604065
Train Loss: 0.9849302768707275
EPOCH 71:
    Batch 0 | Loss: 0.98452228307724
    Batch 1 | Loss: 0.9865127205848694
    Batch 2 | Loss: 0.9807751178741455
    Batch 3 | Loss: 0.9857780337333679
    Batch 4 | Loss: 0.9860823154449463
    Batch 5 | Loss: 0.9883520007133484
    Batch 6 | Loss: 0.9863746762275696
    Batch 7 | Loss: 0.9859872460365295
    Batch 8 | Loss: 0.9837088584899902
Train Loss: 0.9853436946868896
EPOCH 72:
    Batch 0 | Loss: 0.9863513112068176
    Batch 1 | Loss: 0.9846357107162476
    Batch 2 | Loss: 0.9845526218414307
    Batch 3 | Loss: 0.9880362153053284
    Batch 4 | Loss: 0.9816805720329285
    Batch 5 | Loss: 0.9846689105033875
    Batch 6 | Loss: 0.9853724241256714
    Batch 7 | Loss: 0.9842444658279419
    Batch 8 | Loss: 0.9904723167419434
Train Loss: 0.9855571389198303
EPOCH 73:
    Batch 0 | Loss: 0.9816825985908508
    Batch 1 | Loss: 0.985008716583252
    Batch 2 | Loss: 0.9872405529022217
    Batch 3 | Loss: 0.9841602444648743
    Batch 4 | Loss: 0.9880571365356445
    Batch 5 | Loss: 0.9855963587760925
    Batch 6 | Loss: 0.9816630482673645
    Batch 7 | Loss: 0.9851951003074646
    Batch 8 | Loss: 0.986566424369812
Train Loss: 0.9850189685821533
EPOCH 74:
    Batch 0 | Loss: 0.9880797266960144
    Batch 1 | Loss: 0.9841681718826294
    Batch 2 | Loss: 0.9841169714927673
    Batch 3 | Loss: 0.9849883317947388
    Batch 4 | Loss: 0.982988178730011
    Batch 5 | Loss: 0.9858298897743225
    Batch 6 | Loss: 0.9846950769424438
    Batch 7 | Loss: 0.984274685382843
    Batch 8 | Loss: 0.9835237860679626
Train Loss: 0.9847405552864075
EPOCH 75:
    Batch 0 | Loss: 0.9853584170341492
    Batch 1 | Loss: 0.9835559129714966
    Batch 2 | Loss: 0.9799572229385376
    Batch 3 | Loss: 0.9821134805679321
    Batch 4 | Loss: 0.9865645170211792
    Batch 5 | Loss: 0.9859639406204224
    Batch 6 | Loss: 0.9846723079681396
    Batch 7 | Loss: 0.9836764931678772
    Batch 8 | Loss: 0.98808354139328
Train Loss: 0.9844383597373962
EPOCH 76:
    Batch 0 | Loss: 0.9869547486305237
    Batch 1 | Loss: 0.9817488193511963
    Batch 2 | Loss: 0.9865738153457642
    Batch 3 | Loss: 0.988558292388916
    Batch 4 | Loss: 0.98268723487854
    Batch 5 | Loss: 0.9860914349555969
    Batch 6 | Loss: 0.9852148294448853
    Batch 7 | Loss: 0.9848571419715881
    Batch 8 | Loss: 0.9831476807594299
Train Loss: 0.9850926995277405
EPOCH 77:
    Batch 0 | Loss: 0.9841716289520264
    Batch 1 | Loss: 0.9810206294059753
    Batch 2 | Loss: 0.9888268709182739
    Batch 3 | Loss: 0.9876663088798523
    Batch 4 | Loss: 0.9876378178596497
    Batch 5 | Loss: 0.9834588170051575
    Batch 6 | Loss: 0.9843400120735168
    Batch 7 | Loss: 0.9799211025238037
    Batch 8 | Loss: 0.9848735928535461
Train Loss: 0.9846575260162354
EPOCH 78:
    Batch 0 | Loss: 0.9798520803451538
    Batch 1 | Loss: 0.983463704586029
    Batch 2 | Loss: 0.9859291911125183
    Batch 3 | Loss: 0.9829205274581909
    Batch 4 | Loss: 0.9832022190093994
    Batch 5 | Loss: 0.9882757663726807
    Batch 6 | Loss: 0.9818782806396484
    Batch 7 | Loss: 0.983725905418396
    Batch 8 | Loss: 0.98213791847229
Train Loss: 0.9834873676300049
EPOCH 79:
    Batch 0 | Loss: 0.9857301712036133
    Batch 1 | Loss: 0.986118733882904
    Batch 2 | Loss: 0.9813727736473083
    Batch 3 | Loss: 0.98317551612854
    Batch 4 | Loss: 0.9891560077667236
    Batch 5 | Loss: 0.980772852897644
    Batch 6 | Loss: 0.9842811226844788
    Batch 7 | Loss: 0.9847977757453918
    Batch 8 | Loss: 0.9805920124053955
Train Loss: 0.9839996695518494
EPOCH 80:
    Batch 0 | Loss: 0.9866933822631836
    Batch 1 | Loss: 0.9819421172142029
    Batch 2 | Loss: 0.9846211671829224
    Batch 3 | Loss: 0.9810215830802917
    Batch 4 | Loss: 0.9846680164337158
    Batch 5 | Loss: 0.9855901002883911
    Batch 6 | Loss: 0.986664354801178
    Batch 7 | Loss: 0.9842678904533386
    Batch 8 | Loss: 0.9894996881484985
Train Loss: 0.984996497631073
EPOCH 81:
    Batch 0 | Loss: 0.9811393022537231
    Batch 1 | Loss: 0.9818593859672546
    Batch 2 | Loss: 0.9827464818954468
    Batch 3 | Loss: 0.9821215271949768
    Batch 4 | Loss: 0.9869065880775452
    Batch 5 | Loss: 0.9828808903694153
    Batch 6 | Loss: 0.9865915179252625
    Batch 7 | Loss: 0.9846954941749573
    Batch 8 | Loss: 0.9810919165611267
Train Loss: 0.9833369851112366
EPOCH 82:
    Batch 0 | Loss: 0.9868446588516235
    Batch 1 | Loss: 0.9891847968101501
    Batch 2 | Loss: 0.9838915467262268
    Batch 3 | Loss: 0.9823681116104126
    Batch 4 | Loss: 0.9839630126953125
    Batch 5 | Loss: 0.9825156331062317
    Batch 6 | Loss: 0.9825395345687866
    Batch 7 | Loss: 0.9842196702957153
    Batch 8 | Loss: 0.9857750535011292
Train Loss: 0.984589159488678
EPOCH 83:
    Batch 0 | Loss: 0.9813987016677856
    Batch 1 | Loss: 0.9854819178581238
    Batch 2 | Loss: 0.9827239513397217
    Batch 3 | Loss: 0.984877347946167
    Batch 4 | Loss: 0.9856014847755432
    Batch 5 | Loss: 0.9860515594482422
    Batch 6 | Loss: 0.985132098197937
    Batch 7 | Loss: 0.9857668280601501
    Batch 8 | Loss: 0.9843917489051819
Train Loss: 0.9846029281616211
EPOCH 84:
    Batch 0 | Loss: 0.9874168634414673
    Batch 1 | Loss: 0.9802102446556091
    Batch 2 | Loss: 0.9837035536766052
    Batch 3 | Loss: 0.9829868674278259
    Batch 4 | Loss: 0.9853722453117371
    Batch 5 | Loss: 0.9792020916938782
    Batch 6 | Loss: 0.983482837677002
    Batch 7 | Loss: 0.9847599864006042
    Batch 8 | Loss: 0.9856254458427429
Train Loss: 0.9836400151252747
EPOCH 85:
    Batch 0 | Loss: 0.9852108955383301
    Batch 1 | Loss: 0.9858945608139038
    Batch 2 | Loss: 0.9831731915473938
    Batch 3 | Loss: 0.9827711582183838
    Batch 4 | Loss: 0.9858123660087585
    Batch 5 | Loss: 0.9874597787857056
    Batch 6 | Loss: 0.9865610599517822
    Batch 7 | Loss: 0.9824293851852417
    Batch 8 | Loss: 0.985334575176239
Train Loss: 0.9849607944488525
EPOCH 86:
    Batch 0 | Loss: 0.9875212907791138
    Batch 1 | Loss: 0.9845107793807983
    Batch 2 | Loss: 0.9853376746177673
    Batch 3 | Loss: 0.9863843321800232
    Batch 4 | Loss: 0.9872405529022217
    Batch 5 | Loss: 0.9844233393669128
    Batch 6 | Loss: 0.9856048226356506
    Batch 7 | Loss: 0.9803472757339478
    Batch 8 | Loss: 0.9861868023872375
Train Loss: 0.9852840900421143
EPOCH 87:
    Batch 0 | Loss: 0.9903990626335144
    Batch 1 | Loss: 0.9857038855552673
    Batch 2 | Loss: 0.9832399487495422
    Batch 3 | Loss: 0.9864704608917236
    Batch 4 | Loss: 0.9790690541267395
    Batch 5 | Loss: 0.9870078563690186
    Batch 6 | Loss: 0.9833897948265076
    Batch 7 | Loss: 0.9855619072914124
    Batch 8 | Loss: 0.9853218197822571
Train Loss: 0.9851292371749878
EPOCH 88:
    Batch 0 | Loss: 0.9853598475456238
    Batch 1 | Loss: 0.9830932021141052
    Batch 2 | Loss: 0.9862061738967896
    Batch 3 | Loss: 0.9807918071746826
    Batch 4 | Loss: 0.9815033674240112
    Batch 5 | Loss: 0.987429678440094
    Batch 6 | Loss: 0.987106442451477
    Batch 7 | Loss: 0.9809492826461792
    Batch 8 | Loss: 0.9852949976921082
Train Loss: 0.9841927289962769
EPOCH 89:
    Batch 0 | Loss: 0.9858290553092957
    Batch 1 | Loss: 0.9829014539718628
    Batch 2 | Loss: 0.9853411316871643
    Batch 3 | Loss: 0.9860901236534119
    Batch 4 | Loss: 0.9818670153617859
    Batch 5 | Loss: 0.9829005002975464
    Batch 6 | Loss: 0.9832590818405151
    Batch 7 | Loss: 0.9826126098632812
    Batch 8 | Loss: 0.9854577779769897
Train Loss: 0.984028697013855
EPOCH 90:
    Batch 0 | Loss: 0.983000636100769
    Batch 1 | Loss: 0.9839379787445068
    Batch 2 | Loss: 0.986551821231842
    Batch 3 | Loss: 0.98291015625
    Batch 4 | Loss: 0.987032949924469
    Batch 5 | Loss: 0.987922728061676
    Batch 6 | Loss: 0.9867221713066101
    Batch 7 | Loss: 0.9827020764350891
    Batch 8 | Loss: 0.9869568347930908
Train Loss: 0.9853041768074036
EPOCH 91:
    Batch 0 | Loss: 0.9884095191955566
    Batch 1 | Loss: 0.9847584962844849
    Batch 2 | Loss: 0.9864397644996643
    Batch 3 | Loss: 0.9867271184921265
    Batch 4 | Loss: 0.9835594892501831
    Batch 5 | Loss: 0.9793863296508789
    Batch 6 | Loss: 0.9839054942131042
    Batch 7 | Loss: 0.9856668710708618
    Batch 8 | Loss: 0.9836993217468262
Train Loss: 0.9847280979156494
EPOCH 92:
    Batch 0 | Loss: 0.9832153916358948
    Batch 1 | Loss: 0.9839000105857849
    Batch 2 | Loss: 0.9855681657791138
    Batch 3 | Loss: 0.9858286380767822
    Batch 4 | Loss: 0.9839532375335693
    Batch 5 | Loss: 0.9819303750991821
    Batch 6 | Loss: 0.9865935444831848
    Batch 7 | Loss: 0.9856340289115906
    Batch 8 | Loss: 0.9823576211929321
Train Loss: 0.9843312501907349
EPOCH 93:
    Batch 0 | Loss: 0.9863747358322144
    Batch 1 | Loss: 0.988235592842102
    Batch 2 | Loss: 0.9821186065673828
    Batch 3 | Loss: 0.9828447699546814
    Batch 4 | Loss: 0.9861235022544861
    Batch 5 | Loss: 0.9841812252998352
    Batch 6 | Loss: 0.9834550619125366
    Batch 7 | Loss: 0.9836286902427673
    Batch 8 | Loss: 0.9782870411872864
Train Loss: 0.9839165806770325
EPOCH 94:
    Batch 0 | Loss: 0.9920604228973389
    Batch 1 | Loss: 0.9827113151550293
    Batch 2 | Loss: 0.9806275367736816
    Batch 3 | Loss: 0.982113242149353
    Batch 4 | Loss: 0.982546865940094
    Batch 5 | Loss: 0.9839422106742859
    Batch 6 | Loss: 0.9819704294204712
    Batch 7 | Loss: 0.9814348816871643
    Batch 8 | Loss: 0.9825328588485718
Train Loss: 0.9833266139030457
EPOCH 95:
    Batch 0 | Loss: 0.9837449789047241
    Batch 1 | Loss: 0.9871740937232971
    Batch 2 | Loss: 0.9846447706222534
    Batch 3 | Loss: 0.9884576201438904
    Batch 4 | Loss: 0.9854364991188049
    Batch 5 | Loss: 0.9826486110687256
    Batch 6 | Loss: 0.9818514585494995
    Batch 7 | Loss: 0.9874929189682007
    Batch 8 | Loss: 0.985360324382782
Train Loss: 0.9852012991905212
EPOCH 96:
    Batch 0 | Loss: 0.986060380935669
    Batch 1 | Loss: 0.981132447719574
    Batch 2 | Loss: 0.9863961935043335
    Batch 3 | Loss: 0.9869126081466675
    Batch 4 | Loss: 0.9864567518234253
    Batch 5 | Loss: 0.9879513382911682
    Batch 6 | Loss: 0.989758312702179
    Batch 7 | Loss: 0.9815084338188171
    Batch 8 | Loss: 0.9879831671714783
Train Loss: 0.9860177636146545
EPOCH 97:
    Batch 0 | Loss: 0.986644446849823
    Batch 1 | Loss: 0.9833897948265076
    Batch 2 | Loss: 0.983828604221344
    Batch 3 | Loss: 0.9847612977027893
    Batch 4 | Loss: 0.9840869307518005
    Batch 5 | Loss: 0.9858121871948242
    Batch 6 | Loss: 0.9837108850479126
    Batch 7 | Loss: 0.9855005741119385
    Batch 8 | Loss: 0.9831639528274536
Train Loss: 0.9845442175865173
EPOCH 98:
    Batch 0 | Loss: 0.9875335097312927
    Batch 1 | Loss: 0.9804213643074036
    Batch 2 | Loss: 0.9846263527870178
    Batch 3 | Loss: 0.9814828038215637
    Batch 4 | Loss: 0.9828310608863831
    Batch 5 | Loss: 0.9825405478477478
    Batch 6 | Loss: 0.9851853847503662
    Batch 7 | Loss: 0.9815413355827332
    Batch 8 | Loss: 0.9818744659423828
Train Loss: 0.9831150770187378
EPOCH 99:
    Batch 0 | Loss: 0.9836454391479492
    Batch 1 | Loss: 0.9811702966690063
    Batch 2 | Loss: 0.9850731492042542
    Batch 3 | Loss: 0.9846676588058472
    Batch 4 | Loss: 0.9858081936836243
    Batch 5 | Loss: 0.9805912375450134
    Batch 6 | Loss: 0.9852446913719177
    Batch 7 | Loss: 0.9876813292503357
    Batch 8 | Loss: 0.9847200512886047
Train Loss: 0.9842891693115234
EPOCH 100:
    Batch 0 | Loss: 0.9831781387329102
    Batch 1 | Loss: 0.9821773767471313
    Batch 2 | Loss: 0.98785799741745
    Batch 3 | Loss: 0.9856917858123779
    Batch 4 | Loss: 0.980828583240509
    Batch 5 | Loss: 0.9823943972587585
    Batch 6 | Loss: 0.9816901087760925
    Batch 7 | Loss: 0.9846398830413818
    Batch 8 | Loss: 0.9858946800231934
Train Loss: 0.9838169813156128
