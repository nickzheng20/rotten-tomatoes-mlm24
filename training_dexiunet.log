/Users/radia78/Projects/MLM24/rotten-tomatoes-mlm24/mlm24-venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
/Users/radia78/Projects/MLM24/rotten-tomatoes-mlm24/mlm24-venv/lib/python3.9/site-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
EPOCH 1:
    Batch 0 | Loss: 0.9863494038581848
    Batch 1 | Loss: 0.9822515249252319
    Batch 2 | Loss: 0.982832670211792
    Batch 3 | Loss: 0.9866190552711487
    Batch 4 | Loss: 0.9811957478523254
    Batch 5 | Loss: 0.9826996326446533
    Batch 6 | Loss: 0.98175448179245
    Batch 7 | Loss: 0.9794754385948181
    Batch 8 | Loss: 0.9828999638557434
Train Loss: 0.9828975200653076
EPOCH 2:
    Batch 0 | Loss: 0.9849903583526611
    Batch 1 | Loss: 0.9799829721450806
    Batch 2 | Loss: 0.9783727526664734
    Batch 3 | Loss: 0.9765582084655762
    Batch 4 | Loss: 0.974030613899231
    Batch 5 | Loss: 0.9785510301589966
    Batch 6 | Loss: 0.9821927547454834
    Batch 7 | Loss: 0.9718878865242004
    Batch 8 | Loss: 0.9720771312713623
Train Loss: 0.9776270389556885
EPOCH 3:
    Batch 0 | Loss: 0.9814603328704834
    Batch 1 | Loss: 0.9708864688873291
    Batch 2 | Loss: 0.9779313802719116
    Batch 3 | Loss: 0.9832305908203125
    Batch 4 | Loss: 0.9756742119789124
    Batch 5 | Loss: 0.9696446657180786
    Batch 6 | Loss: 0.9808660745620728
    Batch 7 | Loss: 0.9832626581192017
    Batch 8 | Loss: 0.9727182388305664
Train Loss: 0.9772971272468567
EPOCH 4:
    Batch 0 | Loss: 0.9728970527648926
    Batch 1 | Loss: 0.9735841155052185
    Batch 2 | Loss: 0.9733713865280151
    Batch 3 | Loss: 0.9704881310462952
    Batch 4 | Loss: 0.9701430797576904
    Batch 5 | Loss: 0.9758617877960205
    Batch 6 | Loss: 0.976487398147583
    Batch 7 | Loss: 0.9712775945663452
    Batch 8 | Loss: 0.9675576686859131
Train Loss: 0.9724075794219971
EPOCH 5:
    Batch 0 | Loss: 0.974007785320282
    Batch 1 | Loss: 0.9691463112831116
    Batch 2 | Loss: 0.9656606912612915
    Batch 3 | Loss: 0.9765570163726807
    Batch 4 | Loss: 0.9676691293716431
    Batch 5 | Loss: 0.9688131809234619
    Batch 6 | Loss: 0.9691752195358276
    Batch 7 | Loss: 0.9725725650787354
    Batch 8 | Loss: 0.9673177599906921
Train Loss: 0.9701021909713745
EPOCH 6:
    Batch 0 | Loss: 0.963409423828125
    Batch 1 | Loss: 0.97795170545578
    Batch 2 | Loss: 0.9689865112304688
    Batch 3 | Loss: 0.9718553423881531
    Batch 4 | Loss: 0.9628410935401917
    Batch 5 | Loss: 0.9701772332191467
    Batch 6 | Loss: 0.9681652784347534
    Batch 7 | Loss: 0.9690202474594116
    Batch 8 | Loss: 0.9616146087646484
Train Loss: 0.9682246446609497
EPOCH 7:
    Batch 0 | Loss: 0.9687608480453491
    Batch 1 | Loss: 0.9701301455497742
    Batch 2 | Loss: 0.9655842781066895
    Batch 3 | Loss: 0.9603858590126038
    Batch 4 | Loss: 0.9622250199317932
    Batch 5 | Loss: 0.965366542339325
    Batch 6 | Loss: 0.9783305525779724
    Batch 7 | Loss: 0.96980220079422
    Batch 8 | Loss: 0.9720958471298218
Train Loss: 0.9680756330490112
EPOCH 8:
    Batch 0 | Loss: 0.967257559299469
    Batch 1 | Loss: 0.9641255736351013
    Batch 2 | Loss: 0.9763856530189514
    Batch 3 | Loss: 0.961754560470581
    Batch 4 | Loss: 0.9626600742340088
    Batch 5 | Loss: 0.9650989174842834
    Batch 6 | Loss: 0.969529926776886
    Batch 7 | Loss: 0.9669674038887024
    Batch 8 | Loss: 0.9603317975997925
Train Loss: 0.9660124182701111
EPOCH 9:
    Batch 0 | Loss: 0.9620259404182434
    Batch 1 | Loss: 0.9621902704238892
    Batch 2 | Loss: 0.9667912721633911
    Batch 3 | Loss: 0.9625260829925537
    Batch 4 | Loss: 0.9613961577415466
    Batch 5 | Loss: 0.9679951071739197
    Batch 6 | Loss: 0.9689863324165344
    Batch 7 | Loss: 0.959642767906189
    Batch 8 | Loss: 0.9585330486297607
Train Loss: 0.9633429646492004
EPOCH 10:
    Batch 0 | Loss: 0.9635557532310486
    Batch 1 | Loss: 0.9656017422676086
    Batch 2 | Loss: 0.9643036127090454
    Batch 3 | Loss: 0.960534930229187
    Batch 4 | Loss: 0.9584701657295227
    Batch 5 | Loss: 0.9609860777854919
    Batch 6 | Loss: 0.9622133374214172
    Batch 7 | Loss: 0.9559536576271057
    Batch 8 | Loss: 0.956170916557312
Train Loss: 0.9608656764030457
EPOCH 11:
    Batch 0 | Loss: 0.960509717464447
    Batch 1 | Loss: 0.9668079018592834
    Batch 2 | Loss: 0.9676311016082764
    Batch 3 | Loss: 0.9618997573852539
    Batch 4 | Loss: 0.9636989235877991
    Batch 5 | Loss: 0.956707775592804
    Batch 6 | Loss: 0.9530519843101501
    Batch 7 | Loss: 0.9650121331214905
    Batch 8 | Loss: 0.9592716097831726
Train Loss: 0.9616212844848633
EPOCH 12:
    Batch 0 | Loss: 0.9573381543159485
    Batch 1 | Loss: 0.9566101431846619
    Batch 2 | Loss: 0.9560188055038452
    Batch 3 | Loss: 0.9591726064682007
    Batch 4 | Loss: 0.9586104154586792
    Batch 5 | Loss: 0.9657514691352844
    Batch 6 | Loss: 0.9693183898925781
    Batch 7 | Loss: 0.9647142887115479
    Batch 8 | Loss: 0.9619172811508179
Train Loss: 0.9610501527786255
EPOCH 13:
    Batch 0 | Loss: 0.9616658091545105
    Batch 1 | Loss: 0.965423047542572
    Batch 2 | Loss: 0.9612941741943359
    Batch 3 | Loss: 0.945881724357605
    Batch 4 | Loss: 0.9545636177062988
    Batch 5 | Loss: 0.9698436856269836
    Batch 6 | Loss: 0.9591936469078064
    Batch 7 | Loss: 0.9587727189064026
    Batch 8 | Loss: 0.9662488102912903
Train Loss: 0.960320770740509
EPOCH 14:
    Batch 0 | Loss: 0.9520187377929688
    Batch 1 | Loss: 0.9580269455909729
    Batch 2 | Loss: 0.9624776840209961
    Batch 3 | Loss: 0.960756778717041
    Batch 4 | Loss: 0.9684880971908569
    Batch 5 | Loss: 0.9535101056098938
    Batch 6 | Loss: 0.9611163139343262
    Batch 7 | Loss: 0.9505606293678284
    Batch 8 | Loss: 0.9515904784202576
Train Loss: 0.9576161503791809
EPOCH 15:
    Batch 0 | Loss: 0.9577171206474304
    Batch 1 | Loss: 0.960861086845398
    Batch 2 | Loss: 0.9574530124664307
    Batch 3 | Loss: 0.9562476277351379
    Batch 4 | Loss: 0.9616588950157166
    Batch 5 | Loss: 0.9583896994590759
    Batch 6 | Loss: 0.9650831818580627
    Batch 7 | Loss: 0.9550631642341614
    Batch 8 | Loss: 0.9475942850112915
Train Loss: 0.9577853679656982
EPOCH 16:
    Batch 0 | Loss: 0.9618805646896362
    Batch 1 | Loss: 0.9671241044998169
    Batch 2 | Loss: 0.9584834575653076
    Batch 3 | Loss: 0.9555974006652832
    Batch 4 | Loss: 0.948533296585083
    Batch 5 | Loss: 0.9638704061508179
    Batch 6 | Loss: 0.9609991312026978
    Batch 7 | Loss: 0.9552397727966309
    Batch 8 | Loss: 0.9629154205322266
Train Loss: 0.9594048261642456
EPOCH 17:
    Batch 0 | Loss: 0.9572385549545288
    Batch 1 | Loss: 0.9576216340065002
    Batch 2 | Loss: 0.9523723125457764
    Batch 3 | Loss: 0.9618469476699829
    Batch 4 | Loss: 0.9502235651016235
    Batch 5 | Loss: 0.9613814353942871
    Batch 6 | Loss: 0.9561024904251099
    Batch 7 | Loss: 0.9636659622192383
    Batch 8 | Loss: 0.9719605445861816
Train Loss: 0.9591571092605591
EPOCH 18:
    Batch 0 | Loss: 0.9567363858222961
    Batch 1 | Loss: 0.9580568075180054
    Batch 2 | Loss: 0.9563120603561401
    Batch 3 | Loss: 0.9636919498443604
    Batch 4 | Loss: 0.9536206722259521
    Batch 5 | Loss: 0.9517563581466675
    Batch 6 | Loss: 0.9574105143547058
    Batch 7 | Loss: 0.952430784702301
    Batch 8 | Loss: 0.9531949162483215
Train Loss: 0.9559122920036316
EPOCH 19:
    Batch 0 | Loss: 0.9589778780937195
    Batch 1 | Loss: 0.9599735140800476
    Batch 2 | Loss: 0.9678329229354858
    Batch 3 | Loss: 0.9560181498527527
    Batch 4 | Loss: 0.9591478109359741
    Batch 5 | Loss: 0.9455122947692871
    Batch 6 | Loss: 0.9554768204689026
    Batch 7 | Loss: 0.9527237415313721
    Batch 8 | Loss: 0.9510334134101868
Train Loss: 0.9562996625900269
EPOCH 20:
    Batch 0 | Loss: 0.9666085243225098
    Batch 1 | Loss: 0.9505212306976318
    Batch 2 | Loss: 0.9518126249313354
    Batch 3 | Loss: 0.9528886079788208
    Batch 4 | Loss: 0.9624351859092712
    Batch 5 | Loss: 0.943990170955658
    Batch 6 | Loss: 0.9558993577957153
    Batch 7 | Loss: 0.9624935984611511
    Batch 8 | Loss: 0.9423395991325378
Train Loss: 0.9543320536613464
EPOCH 21:
    Batch 0 | Loss: 0.9544699192047119
    Batch 1 | Loss: 0.9671057462692261
    Batch 2 | Loss: 0.9567586779594421
    Batch 3 | Loss: 0.9520283341407776
    Batch 4 | Loss: 0.955551028251648
    Batch 5 | Loss: 0.948361873626709
    Batch 6 | Loss: 0.9528053998947144
    Batch 7 | Loss: 0.9538578391075134
    Batch 8 | Loss: 0.961828351020813
Train Loss: 0.9558629989624023
EPOCH 22:
    Batch 0 | Loss: 0.9501855969429016
    Batch 1 | Loss: 0.951117992401123
    Batch 2 | Loss: 0.951481819152832
    Batch 3 | Loss: 0.9583954215049744
    Batch 4 | Loss: 0.954317569732666
    Batch 5 | Loss: 0.9604666233062744
    Batch 6 | Loss: 0.9568314552307129
    Batch 7 | Loss: 0.9517724514007568
    Batch 8 | Loss: 0.9376481175422668
Train Loss: 0.9524684548377991
EPOCH 23:
    Batch 0 | Loss: 0.9548553824424744
    Batch 1 | Loss: 0.9464238882064819
    Batch 2 | Loss: 0.9582096934318542
    Batch 3 | Loss: 0.9616156816482544
    Batch 4 | Loss: 0.9419209361076355
    Batch 5 | Loss: 0.943059504032135
    Batch 6 | Loss: 0.9606629014015198
    Batch 7 | Loss: 0.9636887311935425
    Batch 8 | Loss: 0.9404463768005371
Train Loss: 0.9523203372955322
EPOCH 24:
    Batch 0 | Loss: 0.9541391134262085
    Batch 1 | Loss: 0.9518170356750488
    Batch 2 | Loss: 0.9504591226577759
    Batch 3 | Loss: 0.9472282528877258
    Batch 4 | Loss: 0.9595127105712891
    Batch 5 | Loss: 0.9441812634468079
    Batch 6 | Loss: 0.9569613337516785
    Batch 7 | Loss: 0.9502995610237122
    Batch 8 | Loss: 0.946462094783783
Train Loss: 0.9512288570404053
EPOCH 25:
    Batch 0 | Loss: 0.9484987258911133
    Batch 1 | Loss: 0.9507877230644226
    Batch 2 | Loss: 0.9433621764183044
    Batch 3 | Loss: 0.9527767896652222
    Batch 4 | Loss: 0.9501266479492188
    Batch 5 | Loss: 0.9629244804382324
    Batch 6 | Loss: 0.9390263557434082
    Batch 7 | Loss: 0.945827841758728
    Batch 8 | Loss: 0.9427971839904785
Train Loss: 0.9484586715698242
EPOCH 26:
    Batch 0 | Loss: 0.9445590972900391
    Batch 1 | Loss: 0.9399029612541199
    Batch 2 | Loss: 0.9570403099060059
    Batch 3 | Loss: 0.9623290300369263
    Batch 4 | Loss: 0.9447891116142273
    Batch 5 | Loss: 0.9411695599555969
    Batch 6 | Loss: 0.9419918060302734
    Batch 7 | Loss: 0.9521479606628418
    Batch 8 | Loss: 0.9349566698074341
Train Loss: 0.9465429782867432
EPOCH 27:
    Batch 0 | Loss: 0.955955982208252
    Batch 1 | Loss: 0.9504371285438538
    Batch 2 | Loss: 0.944351077079773
    Batch 3 | Loss: 0.9565548896789551
    Batch 4 | Loss: 0.9400559067726135
    Batch 5 | Loss: 0.9340475797653198
    Batch 6 | Loss: 0.9125809669494629
    Batch 7 | Loss: 0.9420226216316223
    Batch 8 | Loss: 0.9729547500610352
Train Loss: 0.9454400539398193
EPOCH 28:
    Batch 0 | Loss: 0.948993444442749
    Batch 1 | Loss: 0.939470648765564
    Batch 2 | Loss: 0.9340660572052002
    Batch 3 | Loss: 0.9577775597572327
    Batch 4 | Loss: 0.9584645628929138
    Batch 5 | Loss: 0.9486350417137146
    Batch 6 | Loss: 0.9513905644416809
    Batch 7 | Loss: 0.9522125720977783
    Batch 8 | Loss: 0.9514757990837097
Train Loss: 0.9491652250289917
EPOCH 29:
    Batch 0 | Loss: 0.9219675660133362
    Batch 1 | Loss: 0.9472599029541016
    Batch 2 | Loss: 0.9469572305679321
    Batch 3 | Loss: 0.9313328266143799
    Batch 4 | Loss: 0.9386600852012634
    Batch 5 | Loss: 0.9524235725402832
    Batch 6 | Loss: 0.9391834735870361
    Batch 7 | Loss: 0.9586271047592163
    Batch 8 | Loss: 0.9204359650611877
Train Loss: 0.9396498203277588
EPOCH 30:
    Batch 0 | Loss: 0.9525505900382996
    Batch 1 | Loss: 0.9390597343444824
    Batch 2 | Loss: 0.9523438811302185
    Batch 3 | Loss: 0.955116331577301
    Batch 4 | Loss: 0.9483490586280823
    Batch 5 | Loss: 0.9151469469070435
    Batch 6 | Loss: 0.9551063179969788
    Batch 7 | Loss: 0.9513916969299316
    Batch 8 | Loss: 0.9413616061210632
Train Loss: 0.9456029534339905
EPOCH 31:
    Batch 0 | Loss: 0.9610603451728821
    Batch 1 | Loss: 0.9507351517677307
    Batch 2 | Loss: 0.9477643966674805
    Batch 3 | Loss: 0.9391339421272278
    Batch 4 | Loss: 0.947897732257843
    Batch 5 | Loss: 0.9373226761817932
    Batch 6 | Loss: 0.9396376013755798
    Batch 7 | Loss: 0.9507617950439453
    Batch 8 | Loss: 0.9611304402351379
Train Loss: 0.9483826756477356
EPOCH 32:
    Batch 0 | Loss: 0.9400524497032166
    Batch 1 | Loss: 0.9504777193069458
    Batch 2 | Loss: 0.9278380870819092
    Batch 3 | Loss: 0.9514937996864319
    Batch 4 | Loss: 0.9284594058990479
    Batch 5 | Loss: 0.9243926405906677
    Batch 6 | Loss: 0.9539216160774231
    Batch 7 | Loss: 0.9529352188110352
    Batch 8 | Loss: 0.940728485584259
Train Loss: 0.9411444067955017
EPOCH 33:
    Batch 0 | Loss: 0.9361822605133057
    Batch 1 | Loss: 0.9336097836494446
    Batch 2 | Loss: 0.9418774247169495
    Batch 3 | Loss: 0.9492747187614441
    Batch 4 | Loss: 0.9405203461647034
    Batch 5 | Loss: 0.9252934455871582
    Batch 6 | Loss: 0.9496570229530334
    Batch 7 | Loss: 0.9517440795898438
    Batch 8 | Loss: 0.9428368806838989
Train Loss: 0.9412217736244202
EPOCH 34:
    Batch 0 | Loss: 0.9498317241668701
    Batch 1 | Loss: 0.9413089752197266
    Batch 2 | Loss: 0.9411147832870483
    Batch 3 | Loss: 0.9424503445625305
    Batch 4 | Loss: 0.9364369511604309
    Batch 5 | Loss: 0.9396569728851318
    Batch 6 | Loss: 0.9566553831100464
    Batch 7 | Loss: 0.9435571432113647
    Batch 8 | Loss: 0.8899885416030884
Train Loss: 0.9378889799118042
EPOCH 35:
    Batch 0 | Loss: 0.888217568397522
    Batch 1 | Loss: 0.9411059021949768
    Batch 2 | Loss: 0.9595489501953125
    Batch 3 | Loss: 0.9423036575317383
    Batch 4 | Loss: 0.9545894265174866
    Batch 5 | Loss: 0.9242738485336304
    Batch 6 | Loss: 0.9506757259368896
    Batch 7 | Loss: 0.9485617876052856
    Batch 8 | Loss: 0.9621555805206299
Train Loss: 0.9412702918052673
EPOCH 36:
    Batch 0 | Loss: 0.9557164907455444
    Batch 1 | Loss: 0.9105252623558044
    Batch 2 | Loss: 0.9579053521156311
    Batch 3 | Loss: 0.8934094309806824
    Batch 4 | Loss: 0.944760799407959
    Batch 5 | Loss: 0.9163817167282104
    Batch 6 | Loss: 0.9530631303787231
    Batch 7 | Loss: 0.9478452801704407
    Batch 8 | Loss: 0.9463993906974792
Train Loss: 0.936223030090332
EPOCH 37:
    Batch 0 | Loss: 0.9425382018089294
    Batch 1 | Loss: 0.9375174641609192
    Batch 2 | Loss: 0.9336035251617432
    Batch 3 | Loss: 0.9551982879638672
    Batch 4 | Loss: 0.9326559901237488
    Batch 5 | Loss: 0.9451418519020081
    Batch 6 | Loss: 0.9458509683609009
    Batch 7 | Loss: 0.9434292316436768
    Batch 8 | Loss: 0.8629506826400757
Train Loss: 0.9332095384597778
EPOCH 38:
    Batch 0 | Loss: 0.9434822797775269
    Batch 1 | Loss: 0.8950427174568176
    Batch 2 | Loss: 0.9336733222007751
    Batch 3 | Loss: 0.9221166372299194
    Batch 4 | Loss: 0.9238731265068054
    Batch 5 | Loss: 0.93373042345047
    Batch 6 | Loss: 0.9497385621070862
    Batch 7 | Loss: 0.8914453387260437
    Batch 8 | Loss: 0.9301514029502869
Train Loss: 0.9248059391975403
EPOCH 39:
    Batch 0 | Loss: 0.9381256103515625
    Batch 1 | Loss: 0.8849243521690369
    Batch 2 | Loss: 0.915959894657135
    Batch 3 | Loss: 0.9418401718139648
    Batch 4 | Loss: 0.9524955749511719
    Batch 5 | Loss: 0.9519771337509155
    Batch 6 | Loss: 0.95147305727005
    Batch 7 | Loss: 0.9352188110351562
    Batch 8 | Loss: 0.9480793476104736
Train Loss: 0.9355660676956177
EPOCH 40:
    Batch 0 | Loss: 0.8994370698928833
    Batch 1 | Loss: 0.910261869430542
    Batch 2 | Loss: 0.9234035611152649
    Batch 3 | Loss: 0.9452437162399292
    Batch 4 | Loss: 0.9256369471549988
    Batch 5 | Loss: 0.9397529363632202
    Batch 6 | Loss: 0.9279183745384216
    Batch 7 | Loss: 0.9244336485862732
    Batch 8 | Loss: 0.9469947814941406
Train Loss: 0.9270091652870178
EPOCH 41:
    Batch 0 | Loss: 0.8793256878852844
    Batch 1 | Loss: 0.941180944442749
    Batch 2 | Loss: 0.9509928226470947
    Batch 3 | Loss: 0.9518736600875854
    Batch 4 | Loss: 0.9232730865478516
    Batch 5 | Loss: 0.9162188768386841
    Batch 6 | Loss: 0.9455227255821228
    Batch 7 | Loss: 0.931343674659729
    Batch 8 | Loss: 0.9307741522789001
Train Loss: 0.9300561547279358
EPOCH 42:
    Batch 0 | Loss: 0.9151363372802734
    Batch 1 | Loss: 0.9472892880439758
    Batch 2 | Loss: 0.8989521861076355
    Batch 3 | Loss: 0.9457302689552307
    Batch 4 | Loss: 0.8873549103736877
    Batch 5 | Loss: 0.9446261525154114
    Batch 6 | Loss: 0.9299011826515198
    Batch 7 | Loss: 0.9088426232337952
    Batch 8 | Loss: 0.9586815237998962
Train Loss: 0.9262793660163879
EPOCH 43:
    Batch 0 | Loss: 0.9260441064834595
    Batch 1 | Loss: 0.9450684785842896
    Batch 2 | Loss: 0.9313981533050537
    Batch 3 | Loss: 0.896470844745636
    Batch 4 | Loss: 0.9584194421768188
    Batch 5 | Loss: 0.8861050009727478
    Batch 6 | Loss: 0.9479888677597046
    Batch 7 | Loss: 0.8947973847389221
    Batch 8 | Loss: 0.8452879786491394
Train Loss: 0.9146201014518738
EPOCH 44:
    Batch 0 | Loss: 0.9374071359634399
    Batch 1 | Loss: 0.9500579237937927
    Batch 2 | Loss: 0.9237530827522278
    Batch 3 | Loss: 0.8965728282928467
    Batch 4 | Loss: 0.9181700944900513
    Batch 5 | Loss: 0.8689444065093994
    Batch 6 | Loss: 0.926500141620636
    Batch 7 | Loss: 0.9215819835662842
    Batch 8 | Loss: 0.9607880115509033
Train Loss: 0.9226417541503906
EPOCH 45:
    Batch 0 | Loss: 0.9288150668144226
    Batch 1 | Loss: 0.9392722845077515
    Batch 2 | Loss: 0.8796842694282532
    Batch 3 | Loss: 0.9418718218803406
    Batch 4 | Loss: 0.9339524507522583
    Batch 5 | Loss: 0.9005849361419678
    Batch 6 | Loss: 0.9400216937065125
    Batch 7 | Loss: 0.858757734298706
    Batch 8 | Loss: 0.9406503438949585
Train Loss: 0.9181788563728333
EPOCH 46:
    Batch 0 | Loss: 0.9345828294754028
    Batch 1 | Loss: 0.8880273699760437
    Batch 2 | Loss: 0.8845292329788208
    Batch 3 | Loss: 0.9350091814994812
    Batch 4 | Loss: 0.827347457408905
    Batch 5 | Loss: 0.952764093875885
    Batch 6 | Loss: 0.9526639580726624
    Batch 7 | Loss: 0.879859209060669
    Batch 8 | Loss: 0.9572077989578247
Train Loss: 0.9124434590339661
EPOCH 47:
    Batch 0 | Loss: 0.9100750684738159
    Batch 1 | Loss: 0.8998298645019531
    Batch 2 | Loss: 0.884858250617981
    Batch 3 | Loss: 0.9460806250572205
    Batch 4 | Loss: 0.923214316368103
    Batch 5 | Loss: 0.9038429260253906
    Batch 6 | Loss: 0.8392176032066345
    Batch 7 | Loss: 0.9316509962081909
    Batch 8 | Loss: 0.9621081352233887
Train Loss: 0.9112086892127991
EPOCH 48:
    Batch 0 | Loss: 0.9050721526145935
    Batch 1 | Loss: 0.9220370054244995
    Batch 2 | Loss: 0.8867895603179932
    Batch 3 | Loss: 0.891776978969574
    Batch 4 | Loss: 0.9101977944374084
    Batch 5 | Loss: 0.9061904549598694
    Batch 6 | Loss: 0.876227855682373
    Batch 7 | Loss: 0.9282947182655334
    Batch 8 | Loss: 0.9389539957046509
Train Loss: 0.907282292842865
EPOCH 49:
    Batch 0 | Loss: 0.88111811876297
    Batch 1 | Loss: 0.9339260458946228
    Batch 2 | Loss: 0.9487873315811157
    Batch 3 | Loss: 0.935942530632019
    Batch 4 | Loss: 0.9662817120552063
    Batch 5 | Loss: 0.8445534706115723
    Batch 6 | Loss: 0.8812728524208069
    Batch 7 | Loss: 0.9126228094100952
    Batch 8 | Loss: 0.7763058543205261
Train Loss: 0.8978679180145264
EPOCH 50:
    Batch 0 | Loss: 0.8590331077575684
    Batch 1 | Loss: 0.9140608310699463
    Batch 2 | Loss: 0.9451693892478943
    Batch 3 | Loss: 0.9334151744842529
    Batch 4 | Loss: 0.9335418343544006
    Batch 5 | Loss: 0.8722389936447144
    Batch 6 | Loss: 0.9374244213104248
    Batch 7 | Loss: 0.9297565817832947
    Batch 8 | Loss: 0.9496570825576782
Train Loss: 0.9193664193153381
EPOCH 51:
    Batch 0 | Loss: 0.8881211876869202
    Batch 1 | Loss: 0.8757628202438354
    Batch 2 | Loss: 0.8336471915245056
    Batch 3 | Loss: 0.9513304829597473
    Batch 4 | Loss: 0.9394147396087646
    Batch 5 | Loss: 0.9176774621009827
    Batch 6 | Loss: 0.8444006443023682
    Batch 7 | Loss: 0.8546890020370483
    Batch 8 | Loss: 0.8953279256820679
Train Loss: 0.8889302015304565
EPOCH 52:
    Batch 0 | Loss: 0.9446739554405212
    Batch 1 | Loss: 0.9150087833404541
    Batch 2 | Loss: 0.9157800674438477
    Batch 3 | Loss: 0.8976300954818726
    Batch 4 | Loss: 0.8757681250572205
    Batch 5 | Loss: 0.9160916805267334
    Batch 6 | Loss: 0.9403637647628784
    Batch 7 | Loss: 0.9116122126579285
    Batch 8 | Loss: 0.9471855759620667
Train Loss: 0.9182349443435669
EPOCH 53:
    Batch 0 | Loss: 0.9216213822364807
    Batch 1 | Loss: 0.949508547782898
    Batch 2 | Loss: 0.9613959789276123
    Batch 3 | Loss: 0.9429260492324829
    Batch 4 | Loss: 0.9578255414962769
    Batch 5 | Loss: 0.9135278463363647
    Batch 6 | Loss: 0.8647710680961609
    Batch 7 | Loss: 0.9334138631820679
    Batch 8 | Loss: 0.8286392688751221
Train Loss: 0.9192922115325928
EPOCH 54:
    Batch 0 | Loss: 0.8862245082855225
    Batch 1 | Loss: 0.8919923901557922
    Batch 2 | Loss: 0.9573857188224792
    Batch 3 | Loss: 0.9275646209716797
    Batch 4 | Loss: 0.9107356667518616
    Batch 5 | Loss: 0.9453492760658264
    Batch 6 | Loss: 0.8525636196136475
    Batch 7 | Loss: 0.7997105121612549
    Batch 8 | Loss: 0.9229734539985657
Train Loss: 0.89938884973526
EPOCH 55:
    Batch 0 | Loss: 0.932463526725769
    Batch 1 | Loss: 0.9135605096817017
    Batch 2 | Loss: 0.931442141532898
    Batch 3 | Loss: 0.88676917552948
    Batch 4 | Loss: 0.9394997954368591
    Batch 5 | Loss: 0.8968433141708374
    Batch 6 | Loss: 0.9418075084686279
    Batch 7 | Loss: 0.9566417336463928
    Batch 8 | Loss: 0.9324672222137451
Train Loss: 0.9257217049598694
EPOCH 56:
    Batch 0 | Loss: 0.9088925123214722
    Batch 1 | Loss: 0.7856693863868713
    Batch 2 | Loss: 0.9062207937240601
    Batch 3 | Loss: 0.916836142539978
    Batch 4 | Loss: 0.8626592755317688
    Batch 5 | Loss: 0.8597660064697266
    Batch 6 | Loss: 0.8410074710845947
    Batch 7 | Loss: 0.9365541338920593
    Batch 8 | Loss: 0.9260779619216919
Train Loss: 0.8826315402984619
EPOCH 57:
    Batch 0 | Loss: 0.8319113254547119
    Batch 1 | Loss: 0.877788245677948
    Batch 2 | Loss: 0.8346437215805054
    Batch 3 | Loss: 0.7865867614746094
    Batch 4 | Loss: 0.9331996440887451
    Batch 5 | Loss: 0.8783127069473267
    Batch 6 | Loss: 0.892938494682312
    Batch 7 | Loss: 0.9425871968269348
    Batch 8 | Loss: 0.8936973214149475
Train Loss: 0.8746294975280762
EPOCH 58:
    Batch 0 | Loss: 0.9391830563545227
    Batch 1 | Loss: 0.8838930130004883
    Batch 2 | Loss: 0.8918963670730591
    Batch 3 | Loss: 0.9427946209907532
    Batch 4 | Loss: 0.8929091691970825
    Batch 5 | Loss: 0.9232307076454163
    Batch 6 | Loss: 0.9347884654998779
    Batch 7 | Loss: 0.8202722668647766
    Batch 8 | Loss: 0.8082650899887085
Train Loss: 0.893025815486908
EPOCH 59:
    Batch 0 | Loss: 0.945871114730835
    Batch 1 | Loss: 0.9335665106773376
    Batch 2 | Loss: 0.8580102324485779
    Batch 3 | Loss: 0.8528905510902405
    Batch 4 | Loss: 0.8795165419578552
    Batch 5 | Loss: 0.9576627612113953
    Batch 6 | Loss: 0.9414165019989014
    Batch 7 | Loss: 0.9089104533195496
    Batch 8 | Loss: 0.7705341577529907
Train Loss: 0.8942642211914062
EPOCH 60:
    Batch 0 | Loss: 0.9036609530448914
    Batch 1 | Loss: 0.9210282564163208
    Batch 2 | Loss: 0.7941427230834961
    Batch 3 | Loss: 0.9392904043197632
    Batch 4 | Loss: 0.9457526206970215
    Batch 5 | Loss: 0.9299921989440918
    Batch 6 | Loss: 0.8405512571334839
    Batch 7 | Loss: 0.806125283241272
    Batch 8 | Loss: 0.9460775256156921
Train Loss: 0.891846776008606
EPOCH 61:
    Batch 0 | Loss: 0.9521481394767761
    Batch 1 | Loss: 0.8913869857788086
    Batch 2 | Loss: 0.8528249859809875
    Batch 3 | Loss: 0.8358819484710693
    Batch 4 | Loss: 0.9306252002716064
    Batch 5 | Loss: 0.8841206431388855
    Batch 6 | Loss: 0.9452160596847534
    Batch 7 | Loss: 0.960593044757843
    Batch 8 | Loss: 0.9494617581367493
Train Loss: 0.9113621115684509
EPOCH 62:
    Batch 0 | Loss: 0.9388501048088074
    Batch 1 | Loss: 0.9189767837524414
    Batch 2 | Loss: 0.8279637694358826
    Batch 3 | Loss: 0.9355367422103882
    Batch 4 | Loss: 0.9139866828918457
    Batch 5 | Loss: 0.9403456449508667
    Batch 6 | Loss: 0.8862417936325073
    Batch 7 | Loss: 0.9186871647834778
    Batch 8 | Loss: 0.9422919154167175
Train Loss: 0.9136534929275513
EPOCH 63:
    Batch 0 | Loss: 0.889777421951294
    Batch 1 | Loss: 0.9310548305511475
    Batch 2 | Loss: 0.8845413327217102
    Batch 3 | Loss: 0.9246006011962891
    Batch 4 | Loss: 0.9126623272895813
    Batch 5 | Loss: 0.9501505494117737
    Batch 6 | Loss: 0.8664864301681519
    Batch 7 | Loss: 0.8297692537307739
    Batch 8 | Loss: 0.9025802612304688
Train Loss: 0.8990692496299744
EPOCH 64:
    Batch 0 | Loss: 0.804099440574646
    Batch 1 | Loss: 0.8925268054008484
    Batch 2 | Loss: 0.8552034497261047
    Batch 3 | Loss: 0.9439966678619385
    Batch 4 | Loss: 0.9235056638717651
    Batch 5 | Loss: 0.9413618445396423
    Batch 6 | Loss: 0.799879789352417
    Batch 7 | Loss: 0.7950148582458496
    Batch 8 | Loss: 0.9363275766372681
Train Loss: 0.8768795728683472
EPOCH 65:
    Batch 0 | Loss: 0.8621772527694702
    Batch 1 | Loss: 0.9268526434898376
    Batch 2 | Loss: 0.8322591781616211
    Batch 3 | Loss: 0.8466368913650513
    Batch 4 | Loss: 0.943599283695221
    Batch 5 | Loss: 0.9201699495315552
    Batch 6 | Loss: 0.9229291081428528
    Batch 7 | Loss: 0.9143431186676025
    Batch 8 | Loss: 0.7930377125740051
Train Loss: 0.8846672177314758
EPOCH 66:
    Batch 0 | Loss: 0.8965673446655273
    Batch 1 | Loss: 0.9057126045227051
    Batch 2 | Loss: 0.9317469000816345
    Batch 3 | Loss: 0.9525511860847473
    Batch 4 | Loss: 0.8726493120193481
    Batch 5 | Loss: 0.915446937084198
    Batch 6 | Loss: 0.9091666340827942
    Batch 7 | Loss: 0.8935204744338989
    Batch 8 | Loss: 0.9302026629447937
Train Loss: 0.9119516015052795
EPOCH 67:
    Batch 0 | Loss: 0.8174735307693481
    Batch 1 | Loss: 0.9327861666679382
    Batch 2 | Loss: 0.877056360244751
    Batch 3 | Loss: 0.9119013547897339
    Batch 4 | Loss: 0.8755789995193481
    Batch 5 | Loss: 0.8928008079528809
    Batch 6 | Loss: 0.9123600721359253
    Batch 7 | Loss: 0.9141325950622559
    Batch 8 | Loss: 0.8801155686378479
Train Loss: 0.8904673457145691
EPOCH 68:
    Batch 0 | Loss: 0.8152872920036316
    Batch 1 | Loss: 0.9078533053398132
    Batch 2 | Loss: 0.8578903675079346
    Batch 3 | Loss: 0.9432596564292908
    Batch 4 | Loss: 0.9411386847496033
    Batch 5 | Loss: 0.9393842220306396
    Batch 6 | Loss: 0.9416615962982178
    Batch 7 | Loss: 0.8098524212837219
    Batch 8 | Loss: 0.9521483182907104
Train Loss: 0.9009418487548828
EPOCH 69:
    Batch 0 | Loss: 0.9203224778175354
    Batch 1 | Loss: 0.7578397989273071
    Batch 2 | Loss: 0.8589270114898682
    Batch 3 | Loss: 0.9066516160964966
    Batch 4 | Loss: 0.9011670351028442
    Batch 5 | Loss: 0.8759936094284058
    Batch 6 | Loss: 0.8453336358070374
    Batch 7 | Loss: 0.9130911231040955
    Batch 8 | Loss: 0.9143526554107666
Train Loss: 0.8770754337310791
EPOCH 70:
    Batch 0 | Loss: 0.9014213681221008
    Batch 1 | Loss: 0.8531070351600647
    Batch 2 | Loss: 0.943651020526886
    Batch 3 | Loss: 0.9034340977668762
    Batch 4 | Loss: 0.8312305808067322
    Batch 5 | Loss: 0.8504915237426758
    Batch 6 | Loss: 0.8322189450263977
    Batch 7 | Loss: 0.91281658411026
    Batch 8 | Loss: 0.8860827088356018
Train Loss: 0.8793838024139404
EPOCH 71:
    Batch 0 | Loss: 0.8681489825248718
    Batch 1 | Loss: 0.8800554871559143
    Batch 2 | Loss: 0.8283056616783142
    Batch 3 | Loss: 0.8775801062583923
    Batch 4 | Loss: 0.8980989456176758
    Batch 5 | Loss: 0.9342018365859985
    Batch 6 | Loss: 0.8341636657714844
    Batch 7 | Loss: 0.9149465560913086
    Batch 8 | Loss: 0.9473987817764282
Train Loss: 0.8869888782501221
EPOCH 72:
    Batch 0 | Loss: 0.9286836385726929
    Batch 1 | Loss: 0.9612793922424316
    Batch 2 | Loss: 0.8887873291969299
    Batch 3 | Loss: 0.8441510200500488
    Batch 4 | Loss: 0.8618073463439941
    Batch 5 | Loss: 0.8496419787406921
    Batch 6 | Loss: 0.9372757077217102
    Batch 7 | Loss: 0.8742786645889282
    Batch 8 | Loss: 0.9575960636138916
Train Loss: 0.9003890156745911
EPOCH 73:
    Batch 0 | Loss: 0.9158371686935425
    Batch 1 | Loss: 0.9377315044403076
    Batch 2 | Loss: 0.8528876900672913
    Batch 3 | Loss: 0.8854222297668457
    Batch 4 | Loss: 0.9371269941329956
    Batch 5 | Loss: 0.8402592539787292
    Batch 6 | Loss: 0.8098925352096558
    Batch 7 | Loss: 0.9086989760398865
    Batch 8 | Loss: 0.9276671409606934
Train Loss: 0.8906137943267822
EPOCH 74:
    Batch 0 | Loss: 0.9370896816253662
    Batch 1 | Loss: 0.9263163805007935
    Batch 2 | Loss: 0.8819813132286072
    Batch 3 | Loss: 0.8457334041595459
    Batch 4 | Loss: 0.8851454257965088
    Batch 5 | Loss: 0.874275803565979
    Batch 6 | Loss: 0.878180980682373
    Batch 7 | Loss: 0.8259734511375427
    Batch 8 | Loss: 0.9610223770141602
Train Loss: 0.8906353712081909
EPOCH 75:
    Batch 0 | Loss: 0.9359313249588013
    Batch 1 | Loss: 0.8972092270851135
    Batch 2 | Loss: 0.9499993920326233
    Batch 3 | Loss: 0.8474946618080139
    Batch 4 | Loss: 0.8927367329597473
    Batch 5 | Loss: 0.8507552146911621
    Batch 6 | Loss: 0.9200935363769531
    Batch 7 | Loss: 0.9213131070137024
    Batch 8 | Loss: 0.8259451389312744
Train Loss: 0.8934975862503052
EPOCH 76:
    Batch 0 | Loss: 0.882675290107727
    Batch 1 | Loss: 0.9190195798873901
    Batch 2 | Loss: 0.8808322548866272
    Batch 3 | Loss: 0.8082194924354553
    Batch 4 | Loss: 0.9144500494003296
    Batch 5 | Loss: 0.8387480974197388
    Batch 6 | Loss: 0.8242855668067932
    Batch 7 | Loss: 0.9125875234603882
    Batch 8 | Loss: 0.7447818517684937
Train Loss: 0.8583999872207642
EPOCH 77:
    Batch 0 | Loss: 0.8940065503120422
    Batch 1 | Loss: 0.8490267992019653
    Batch 2 | Loss: 0.7932004928588867
    Batch 3 | Loss: 0.9159215688705444
    Batch 4 | Loss: 0.9352080821990967
    Batch 5 | Loss: 0.938391387462616
    Batch 6 | Loss: 0.8831334114074707
    Batch 7 | Loss: 0.8298233151435852
    Batch 8 | Loss: 0.9505628347396851
Train Loss: 0.887697160243988
EPOCH 78:
    Batch 0 | Loss: 0.8770449161529541
    Batch 1 | Loss: 0.8907883167266846
    Batch 2 | Loss: 0.8348842263221741
    Batch 3 | Loss: 0.916060209274292
    Batch 4 | Loss: 0.8658238649368286
    Batch 5 | Loss: 0.8370653986930847
    Batch 6 | Loss: 0.9345654249191284
    Batch 7 | Loss: 0.8262637853622437
    Batch 8 | Loss: 0.8985965251922607
Train Loss: 0.8756769895553589
EPOCH 79:
    Batch 0 | Loss: 0.9097060561180115
    Batch 1 | Loss: 0.8388434052467346
    Batch 2 | Loss: 0.8574690222740173
    Batch 3 | Loss: 0.8295261859893799
    Batch 4 | Loss: 0.9317067861557007
    Batch 5 | Loss: 0.8852204084396362
    Batch 6 | Loss: 0.9198987483978271
    Batch 7 | Loss: 0.9314927458763123
    Batch 8 | Loss: 0.743788480758667
Train Loss: 0.8719613552093506
EPOCH 80:
    Batch 0 | Loss: 0.9229995608329773
    Batch 1 | Loss: 0.8876045942306519
    Batch 2 | Loss: 0.8907576203346252
    Batch 3 | Loss: 0.9453949332237244
    Batch 4 | Loss: 0.8702133297920227
    Batch 5 | Loss: 0.9401482939720154
    Batch 6 | Loss: 0.8367645144462585
    Batch 7 | Loss: 0.8273190259933472
    Batch 8 | Loss: 0.8877423405647278
Train Loss: 0.8898827433586121
EPOCH 81:
    Batch 0 | Loss: 0.8777499794960022
    Batch 1 | Loss: 0.8570688366889954
    Batch 2 | Loss: 0.945280909538269
    Batch 3 | Loss: 0.9269317984580994
    Batch 4 | Loss: 0.7759832143783569
    Batch 5 | Loss: 0.8291975855827332
    Batch 6 | Loss: 0.9384759068489075
    Batch 7 | Loss: 0.8993815779685974
    Batch 8 | Loss: 0.951160728931427
Train Loss: 0.8890255689620972
EPOCH 82:
    Batch 0 | Loss: 0.9462079405784607
    Batch 1 | Loss: 0.920610785484314
    Batch 2 | Loss: 0.9443585872650146
    Batch 3 | Loss: 0.8502418994903564
    Batch 4 | Loss: 0.8995636105537415
    Batch 5 | Loss: 0.8800107836723328
    Batch 6 | Loss: 0.9203791618347168
    Batch 7 | Loss: 0.9401475787162781
    Batch 8 | Loss: 0.9622026085853577
Train Loss: 0.9181913733482361
EPOCH 83:
    Batch 0 | Loss: 0.8107243180274963
    Batch 1 | Loss: 0.9477511644363403
    Batch 2 | Loss: 0.7951045036315918
    Batch 3 | Loss: 0.8524870276451111
    Batch 4 | Loss: 0.858765721321106
    Batch 5 | Loss: 0.8186464309692383
    Batch 6 | Loss: 0.9578698873519897
    Batch 7 | Loss: 0.9279756546020508
    Batch 8 | Loss: 0.8056968450546265
Train Loss: 0.8638913631439209
EPOCH 84:
    Batch 0 | Loss: 0.9029529094696045
    Batch 1 | Loss: 0.9272607564926147
    Batch 2 | Loss: 0.7896225452423096
    Batch 3 | Loss: 0.915676474571228
    Batch 4 | Loss: 0.7875926494598389
    Batch 5 | Loss: 0.910344660282135
    Batch 6 | Loss: 0.8886868357658386
    Batch 7 | Loss: 0.8825156688690186
    Batch 8 | Loss: 0.9349408149719238
Train Loss: 0.8821771144866943
EPOCH 85:
    Batch 0 | Loss: 0.9560128450393677
    Batch 1 | Loss: 0.9462045431137085
    Batch 2 | Loss: 0.9101990461349487
    Batch 3 | Loss: 0.8302589654922485
    Batch 4 | Loss: 0.8304640054702759
    Batch 5 | Loss: 0.882470965385437
    Batch 6 | Loss: 0.846036434173584
    Batch 7 | Loss: 0.925179123878479
    Batch 8 | Loss: 0.8976637125015259
Train Loss: 0.8916099071502686
EPOCH 86:
    Batch 0 | Loss: 0.953122615814209
    Batch 1 | Loss: 0.8949287533760071
    Batch 2 | Loss: 0.9130065441131592
    Batch 3 | Loss: 0.9275804162025452
    Batch 4 | Loss: 0.8139656186103821
    Batch 5 | Loss: 0.9232084155082703
    Batch 6 | Loss: 0.957432746887207
    Batch 7 | Loss: 0.9428989887237549
    Batch 8 | Loss: 0.7449413537979126
Train Loss: 0.8967873454093933
EPOCH 87:
    Batch 0 | Loss: 0.7966721653938293
    Batch 1 | Loss: 0.8665789365768433
    Batch 2 | Loss: 0.9214945435523987
    Batch 3 | Loss: 0.8953182101249695
    Batch 4 | Loss: 0.9592499136924744
    Batch 5 | Loss: 0.9267224073410034
    Batch 6 | Loss: 0.9069609642028809
    Batch 7 | Loss: 0.8259314298629761
    Batch 8 | Loss: 0.964779794216156
Train Loss: 0.8959676027297974
EPOCH 88:
    Batch 0 | Loss: 0.9193390607833862
    Batch 1 | Loss: 0.9288842678070068
    Batch 2 | Loss: 0.9324544668197632
    Batch 3 | Loss: 0.9041851758956909
    Batch 4 | Loss: 0.9507288336753845
    Batch 5 | Loss: 0.8286744356155396
    Batch 6 | Loss: 0.875604510307312
    Batch 7 | Loss: 0.8964765071868896
    Batch 8 | Loss: 0.8598917722702026
Train Loss: 0.8995821475982666
EPOCH 89:
    Batch 0 | Loss: 0.9007865786552429
    Batch 1 | Loss: 0.8847367763519287
    Batch 2 | Loss: 0.9221863746643066
    Batch 3 | Loss: 0.900675892829895
    Batch 4 | Loss: 0.8279014825820923
    Batch 5 | Loss: 0.8188864588737488
    Batch 6 | Loss: 0.9347987771034241
    Batch 7 | Loss: 0.8121184706687927
    Batch 8 | Loss: 0.9196943044662476
Train Loss: 0.8801982998847961
EPOCH 90:
    Batch 0 | Loss: 0.9514567852020264
    Batch 1 | Loss: 0.7443023920059204
    Batch 2 | Loss: 0.8052586317062378
    Batch 3 | Loss: 0.9516049027442932
    Batch 4 | Loss: 0.9383864402770996
    Batch 5 | Loss: 0.9097299575805664
    Batch 6 | Loss: 0.7882209420204163
    Batch 7 | Loss: 0.9461319446563721
    Batch 8 | Loss: 0.9552855491638184
Train Loss: 0.887819766998291
EPOCH 91:
    Batch 0 | Loss: 0.8829687237739563
    Batch 1 | Loss: 0.8268074989318848
    Batch 2 | Loss: 0.8433578610420227
    Batch 3 | Loss: 0.7783474922180176
    Batch 4 | Loss: 0.7989407777786255
    Batch 5 | Loss: 0.7840974926948547
    Batch 6 | Loss: 0.8694957494735718
    Batch 7 | Loss: 0.9353804588317871
    Batch 8 | Loss: 0.8990014791488647
Train Loss: 0.8464886546134949
EPOCH 92:
    Batch 0 | Loss: 0.942613422870636
    Batch 1 | Loss: 0.9102866053581238
    Batch 2 | Loss: 0.9420758485794067
    Batch 3 | Loss: 0.9462920427322388
    Batch 4 | Loss: 0.9332481622695923
    Batch 5 | Loss: 0.8297171592712402
    Batch 6 | Loss: 0.8669740557670593
    Batch 7 | Loss: 0.7928453683853149
    Batch 8 | Loss: 0.9698670506477356
Train Loss: 0.90376877784729
EPOCH 93:
    Batch 0 | Loss: 0.9173026084899902
    Batch 1 | Loss: 0.9025877118110657
    Batch 2 | Loss: 0.9213041067123413
    Batch 3 | Loss: 0.9267911911010742
    Batch 4 | Loss: 0.9200011491775513
    Batch 5 | Loss: 0.8115137219429016
    Batch 6 | Loss: 0.9494061470031738
    Batch 7 | Loss: 0.9438471794128418
    Batch 8 | Loss: 0.9047921895980835
Train Loss: 0.9108384251594543
EPOCH 94:
    Batch 0 | Loss: 0.8524712324142456
    Batch 1 | Loss: 0.8834498524665833
    Batch 2 | Loss: 0.9049907326698303
    Batch 3 | Loss: 0.9193774461746216
    Batch 4 | Loss: 0.7936270236968994
    Batch 5 | Loss: 0.9192309379577637
    Batch 6 | Loss: 0.9432189464569092
    Batch 7 | Loss: 0.9295954704284668
    Batch 8 | Loss: 0.7593844532966614
Train Loss: 0.8783717751502991
EPOCH 95:
    Batch 0 | Loss: 0.8108649253845215
    Batch 1 | Loss: 0.8988776206970215
    Batch 2 | Loss: 0.8203832507133484
    Batch 3 | Loss: 0.9057901501655579
    Batch 4 | Loss: 0.9064663052558899
    Batch 5 | Loss: 0.8297404050827026
    Batch 6 | Loss: 0.9377518892288208
    Batch 7 | Loss: 0.924091100692749
    Batch 8 | Loss: 0.9223123788833618
Train Loss: 0.8840309381484985
EPOCH 96:
    Batch 0 | Loss: 0.7867989540100098
    Batch 1 | Loss: 0.9259657859802246
    Batch 2 | Loss: 0.8743813633918762
    Batch 3 | Loss: 0.9174350500106812
    Batch 4 | Loss: 0.9518105387687683
    Batch 5 | Loss: 0.8228236436843872
    Batch 6 | Loss: 0.8879924416542053
    Batch 7 | Loss: 0.91478031873703
    Batch 8 | Loss: 0.8555656671524048
Train Loss: 0.881950318813324
EPOCH 97:
    Batch 0 | Loss: 0.847740650177002
    Batch 1 | Loss: 0.769942581653595
    Batch 2 | Loss: 0.8492040634155273
    Batch 3 | Loss: 0.814520537853241
    Batch 4 | Loss: 0.8815956711769104
    Batch 5 | Loss: 0.8389886617660522
    Batch 6 | Loss: 0.818635106086731
    Batch 7 | Loss: 0.960962176322937
    Batch 8 | Loss: 0.9111732840538025
Train Loss: 0.8547514081001282
EPOCH 98:
    Batch 0 | Loss: 0.9291418194770813
    Batch 1 | Loss: 0.9390964508056641
    Batch 2 | Loss: 0.8771872520446777
    Batch 3 | Loss: 0.7989381551742554
    Batch 4 | Loss: 0.955809473991394
    Batch 5 | Loss: 0.7500587105751038
    Batch 6 | Loss: 0.8060274720191956
    Batch 7 | Loss: 0.8383886814117432
    Batch 8 | Loss: 0.9210771918296814
Train Loss: 0.8684138655662537
EPOCH 99:
    Batch 0 | Loss: 0.8392143249511719
    Batch 1 | Loss: 0.9018613696098328
    Batch 2 | Loss: 0.8199151754379272
    Batch 3 | Loss: 0.9129477739334106
    Batch 4 | Loss: 0.9336361885070801
    Batch 5 | Loss: 0.9365984201431274
    Batch 6 | Loss: 0.9455574750900269
    Batch 7 | Loss: 0.937482476234436
    Batch 8 | Loss: 0.849575936794281
Train Loss: 0.8974210023880005
EPOCH 100:
    Batch 0 | Loss: 0.896014928817749
    Batch 1 | Loss: 0.8974725604057312
    Batch 2 | Loss: 0.9242995977401733
    Batch 3 | Loss: 0.7945550084114075
    Batch 4 | Loss: 0.7906284332275391
    Batch 5 | Loss: 0.7653650045394897
    Batch 6 | Loss: 0.9538994431495667
    Batch 7 | Loss: 0.7711421847343445
    Batch 8 | Loss: 0.957655668258667
Train Loss: 0.8612258434295654
