digraph {
	graph [size="93.75,93.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	281470657131728 [label="
 ()" fillcolor=darkolivegreen1]
	281470657864288 [label=MeanBackward0]
	281470657870624 -> 281470657864288
	281470657870624 [label=MulBackward0]
	281470681683552 -> 281470657870624
	281470681683552 [label=RsubBackward1]
	281473235041488 -> 281470681683552
	281473235041488 [label=DivBackward0]
	281473235031408 -> 281473235041488
	281473235031408 [label=AddBackward0]
	281470657955680 -> 281473235031408
	281470657955680 [label=SumBackward1]
	281470657801104 -> 281470657955680
	281470657801104 [label=MulBackward0]
	281470657801776 -> 281470657801104
	281470657801776 [label=ViewBackward0]
	281470657801440 -> 281470657801776
	281470657801440 [label=ExpBackward0]
	281470657799808 -> 281470657801440
	281470657799808 [label=LogSigmoidBackward0]
	281470657804368 -> 281470657799808
	281470657804368 [label=ConvolutionBackward0]
	281470658242800 -> 281470657804368
	281470658242800 [label=ReluBackward0]
	281470657955632 -> 281470658242800
	281470657955632 [label=NativeBatchNormBackward0]
	281470658241696 -> 281470657955632
	281470658241696 [label=ConvolutionBackward0]
	281470658242512 -> 281470658241696
	281470658242512 [label=ReluBackward0]
	281470658243328 -> 281470658242512
	281470658243328 [label=NativeBatchNormBackward0]
	281470658243376 -> 281470658243328
	281470658243376 [label=ConvolutionBackward0]
	281470658243760 -> 281470658243376
	281470658243760 [label=UpsampleNearest2DBackward0]
	281470657804320 -> 281470658243760
	281470657804320 [label=ReluBackward0]
	281470658244000 -> 281470657804320
	281470658244000 [label=NativeBatchNormBackward0]
	281470658244096 -> 281470658244000
	281470658244096 [label=ConvolutionBackward0]
	281470658244288 -> 281470658244096
	281470658244288 [label=ReluBackward0]
	281470658244432 -> 281470658244288
	281470658244432 [label=NativeBatchNormBackward0]
	281470658244528 -> 281470658244432
	281470658244528 [label=ConvolutionBackward0]
	281470658244720 -> 281470658244528
	281470658244720 [label=CatBackward0]
	281470658244864 -> 281470658244720
	281470658244864 [label=UpsampleNearest2DBackward0]
	281470658245008 -> 281470658244864
	281470658245008 [label=ReluBackward0]
	281470658245104 -> 281470658245008
	281470658245104 [label=NativeBatchNormBackward0]
	281470658245200 -> 281470658245104
	281470658245200 [label=ConvolutionBackward0]
	281470658245392 -> 281470658245200
	281470658245392 [label=ReluBackward0]
	281470658245536 -> 281470658245392
	281470658245536 [label=NativeBatchNormBackward0]
	281470658245632 -> 281470658245536
	281470658245632 [label=ConvolutionBackward0]
	281470658245824 -> 281470658245632
	281470658245824 [label=CatBackward0]
	281470658245968 -> 281470658245824
	281470658245968 [label=UpsampleNearest2DBackward0]
	281470658246112 -> 281470658245968
	281470658246112 [label=ReluBackward0]
	281470658246208 -> 281470658246112
	281470658246208 [label=NativeBatchNormBackward0]
	281470658246304 -> 281470658246208
	281470658246304 [label=ConvolutionBackward0]
	281470658246496 -> 281470658246304
	281470658246496 [label=ReluBackward0]
	281470658246640 -> 281470658246496
	281470658246640 [label=NativeBatchNormBackward0]
	281470658246736 -> 281470658246640
	281470658246736 [label=ConvolutionBackward0]
	281470658246928 -> 281470658246736
	281470658246928 [label=CatBackward0]
	281470658247072 -> 281470658246928
	281470658247072 [label=UpsampleNearest2DBackward0]
	281470658247216 -> 281470658247072
	281470658247216 [label=ReluBackward0]
	281470658247312 -> 281470658247216
	281470658247312 [label=NativeBatchNormBackward0]
	281470658247408 -> 281470658247312
	281470658247408 [label=ConvolutionBackward0]
	281470658247600 -> 281470658247408
	281470658247600 [label=ReluBackward0]
	281470658247744 -> 281470658247600
	281470658247744 [label=NativeBatchNormBackward0]
	281470658247840 -> 281470658247744
	281470658247840 [label=ConvolutionBackward0]
	281470658248032 -> 281470658247840
	281470658248032 [label=CatBackward0]
	281470658248176 -> 281470658248032
	281470658248176 [label=UpsampleNearest2DBackward0]
	281470658248320 -> 281470658248176
	281470658248320 [label=ReluBackward0]
	281470658248416 -> 281470658248320
	281470658248416 [label=AddBackward0]
	281470658248560 -> 281470658248416
	281470658248560 [label=NativeBatchNormBackward0]
	281470658248656 -> 281470658248560
	281470658248656 [label=ConvolutionBackward0]
	281470658248960 -> 281470658248656
	281470658248960 [label=ReluBackward0]
	281470658249104 -> 281470658248960
	281470658249104 [label=NativeBatchNormBackward0]
	281470658249200 -> 281470658249104
	281470658249200 [label=ConvolutionBackward0]
	281470658248512 -> 281470658249200
	281470658248512 [label=ReluBackward0]
	281470658249488 -> 281470658248512
	281470658249488 [label=AddBackward0]
	281470658249584 -> 281470658249488
	281470658249584 [label=NativeBatchNormBackward0]
	281470658249728 -> 281470658249584
	281470658249728 [label=ConvolutionBackward0]
	281470658249920 -> 281470658249728
	281470658249920 [label=ReluBackward0]
	281470658250064 -> 281470658249920
	281470658250064 [label=NativeBatchNormBackward0]
	281470658250160 -> 281470658250064
	281470658250160 [label=ConvolutionBackward0]
	281470658248128 -> 281470658250160
	281470658248128 [label=ReluBackward0]
	281470658250448 -> 281470658248128
	281470658250448 [label=AddBackward0]
	281470658250544 -> 281470658250448
	281470658250544 [label=NativeBatchNormBackward0]
	281470658250688 -> 281470658250544
	281470658250688 [label=ConvolutionBackward0]
	281470658250880 -> 281470658250688
	281470658250880 [label=ReluBackward0]
	281470658251024 -> 281470658250880
	281470658251024 [label=NativeBatchNormBackward0]
	281470658251120 -> 281470658251024
	281470658251120 [label=ConvolutionBackward0]
	281470658250496 -> 281470658251120
	281470658250496 [label=ReluBackward0]
	281470658251408 -> 281470658250496
	281470658251408 [label=AddBackward0]
	281470658251504 -> 281470658251408
	281470658251504 [label=NativeBatchNormBackward0]
	281470658251696 -> 281470658251504
	281470658251696 [label=ConvolutionBackward0]
	281470658251888 -> 281470658251696
	281470658251888 [label=ReluBackward0]
	281470658252032 -> 281470658251888
	281470658252032 [label=NativeBatchNormBackward0]
	281470658252128 -> 281470658252032
	281470658252128 [label=ConvolutionBackward0]
	281470658247024 -> 281470658252128
	281470658247024 [label=ReluBackward0]
	281470658252416 -> 281470658247024
	281470658252416 [label=AddBackward0]
	281470658252512 -> 281470658252416
	281470658252512 [label=NativeBatchNormBackward0]
	281470658252656 -> 281470658252512
	281470658252656 [label=ConvolutionBackward0]
	281470658252848 -> 281470658252656
	281470658252848 [label=ReluBackward0]
	281470658252992 -> 281470658252848
	281470658252992 [label=NativeBatchNormBackward0]
	281470658253088 -> 281470658252992
	281470658253088 [label=ConvolutionBackward0]
	281470658252464 -> 281470658253088
	281470658252464 [label=ReluBackward0]
	281470658253376 -> 281470658252464
	281470658253376 [label=AddBackward0]
	281470658253472 -> 281470658253376
	281470658253472 [label=NativeBatchNormBackward0]
	281470658253616 -> 281470658253472
	281470658253616 [label=ConvolutionBackward0]
	281470658253808 -> 281470658253616
	281470658253808 [label=ReluBackward0]
	281470658253952 -> 281470658253808
	281470658253952 [label=NativeBatchNormBackward0]
	281470658254048 -> 281470658253952
	281470658254048 [label=ConvolutionBackward0]
	281470658245920 -> 281470658254048
	281470658245920 [label=ReluBackward0]
	281470658254336 -> 281470658245920
	281470658254336 [label=AddBackward0]
	281470658254432 -> 281470658254336
	281470658254432 [label=NativeBatchNormBackward0]
	281470658254576 -> 281470658254432
	281470658254576 [label=ConvolutionBackward0]
	281470658254768 -> 281470658254576
	281470658254768 [label=ReluBackward0]
	281470658254912 -> 281470658254768
	281470658254912 [label=NativeBatchNormBackward0]
	281470658255008 -> 281470658254912
	281470658255008 [label=ConvolutionBackward0]
	281470658254384 -> 281470658255008
	281470658254384 [label=ReluBackward0]
	281470658255392 -> 281470658254384
	281470658255392 [label=AddBackward0]
	281470658255488 -> 281470658255392
	281470658255488 [label=NativeBatchNormBackward0]
	281470658255632 -> 281470658255488
	281470658255632 [label=ConvolutionBackward0]
	281470658255824 -> 281470658255632
	281470658255824 [label=ReluBackward0]
	281470658255968 -> 281470658255824
	281470658255968 [label=NativeBatchNormBackward0]
	281470658256064 -> 281470658255968
	281470658256064 [label=ConvolutionBackward0]
	281470658255440 -> 281470658256064
	281470658255440 [label=MaxPool2DWithIndicesBackward0]
	281470658244816 -> 281470658255440
	281470658244816 [label=ReluBackward0]
	281470658256400 -> 281470658244816
	281470658256400 [label=NativeBatchNormBackward0]
	281470658256496 -> 281470658256400
	281470658256496 [label=ConvolutionBackward0]
	281470658256688 -> 281470658256496
	281471636835216 [label="model.encoder.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	281471636835216 -> 281470658256688
	281470658256688 [label=AccumulateGrad]
	281470658256448 -> 281470658256400
	281471636835136 [label="model.encoder.bn1.weight
 (64)" fillcolor=lightblue]
	281471636835136 -> 281470658256448
	281470658256448 [label=AccumulateGrad]
	281470658256304 -> 281470658256400
	281471636835056 [label="model.encoder.bn1.bias
 (64)" fillcolor=lightblue]
	281471636835056 -> 281470658256304
	281470658256304 [label=AccumulateGrad]
	281470658256256 -> 281470658256064
	281471636834416 [label="model.encoder.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	281471636834416 -> 281470658256256
	281470658256256 [label=AccumulateGrad]
	281470658256016 -> 281470658255968
	281471636834496 [label="model.encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	281471636834496 -> 281470658256016
	281470658256016 [label=AccumulateGrad]
	281470658255872 -> 281470658255968
	281471636834176 [label="model.encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	281471636834176 -> 281470658255872
	281470658255872 [label=AccumulateGrad]
	281470658255776 -> 281470658255632
	281471636833776 [label="model.encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	281471636833776 -> 281470658255776
	281470658255776 [label=AccumulateGrad]
	281470658255584 -> 281470658255488
	281471636833856 [label="model.encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	281471636833856 -> 281470658255584
	281470658255584 [label=AccumulateGrad]
	281470658255536 -> 281470658255488
	281471636833696 [label="model.encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	281471636833696 -> 281470658255536
	281470658255536 [label=AccumulateGrad]
	281470658255440 -> 281470658255392
	281470658255296 -> 281470658255008
	281471636833216 [label="model.encoder.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	281471636833216 -> 281470658255296
	281470658255296 [label=AccumulateGrad]
	281470658254960 -> 281470658254912
	281471636833296 [label="model.encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	281471636833296 -> 281470658254960
	281470658254960 [label=AccumulateGrad]
	281470658254816 -> 281470658254912
	281471636833136 [label="model.encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	281471636833136 -> 281470658254816
	281470658254816 [label=AccumulateGrad]
	281470658254720 -> 281470658254576
	281471636832576 [label="model.encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	281471636832576 -> 281470658254720
	281470658254720 [label=AccumulateGrad]
	281470658254528 -> 281470658254432
	281471636832656 [label="model.encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	281471636832656 -> 281470658254528
	281470658254528 [label=AccumulateGrad]
	281470658254480 -> 281470658254432
	281471636832496 [label="model.encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	281471636832496 -> 281470658254480
	281470658254480 [label=AccumulateGrad]
	281470658254384 -> 281470658254336
	281470658254240 -> 281470658254048
	281471636782320 [label="model.encoder.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	281471636782320 -> 281470658254240
	281470658254240 [label=AccumulateGrad]
	281470658254000 -> 281470658253952
	281471636782480 [label="model.encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	281471636782480 -> 281470658254000
	281470658254000 [label=AccumulateGrad]
	281470658253856 -> 281470658253952
	281471636782240 [label="model.encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	281471636782240 -> 281470658253856
	281470658253856 [label=AccumulateGrad]
	281470658253760 -> 281470658253616
	281471636781520 [label="model.encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	281471636781520 -> 281470658253760
	281470658253760 [label=AccumulateGrad]
	281470658253568 -> 281470658253472
	281471636781600 [label="model.encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	281471636781600 -> 281470658253568
	281470658253568 [label=AccumulateGrad]
	281470658253520 -> 281470658253472
	281471636781440 [label="model.encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	281471636781440 -> 281470658253520
	281470658253520 [label=AccumulateGrad]
	281470658253424 -> 281470658253376
	281470658253424 [label=NativeBatchNormBackward0]
	281470658254288 -> 281470658253424
	281470658254288 [label=ConvolutionBackward0]
	281470658245920 -> 281470658254288
	281470658254192 -> 281470658254288
	281471636835856 [label="model.encoder.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	281471636835856 -> 281470658254192
	281470658254192 [label=AccumulateGrad]
	281470658253904 -> 281470658253424
	281471636783040 [label="model.encoder.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	281471636783040 -> 281470658253904
	281470658253904 [label=AccumulateGrad]
	281470658253664 -> 281470658253424
	281471636782960 [label="model.encoder.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	281471636782960 -> 281470658253664
	281470658253664 [label=AccumulateGrad]
	281470658253280 -> 281470658253088
	281471636780960 [label="model.encoder.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	281471636780960 -> 281470658253280
	281470658253280 [label=AccumulateGrad]
	281470658253040 -> 281470658252992
	281471636781040 [label="model.encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	281471636781040 -> 281470658253040
	281470658253040 [label=AccumulateGrad]
	281470658252896 -> 281470658252992
	281471636782000 [label="model.encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	281471636782000 -> 281470658252896
	281470658252896 [label=AccumulateGrad]
	281470658252800 -> 281470658252656
	281470658028528 [label="model.encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	281470658028528 -> 281470658252800
	281470658252800 [label=AccumulateGrad]
	281470658252608 -> 281470658252512
	281471636780320 [label="model.encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	281471636780320 -> 281470658252608
	281470658252608 [label=AccumulateGrad]
	281470658252560 -> 281470658252512
	281470658028608 [label="model.encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	281470658028608 -> 281470658252560
	281470658252560 [label=AccumulateGrad]
	281470658252464 -> 281470658252416
	281470658252320 -> 281470658252128
	281470658029888 [label="model.encoder.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	281470658029888 -> 281470658252320
	281470658252320 [label=AccumulateGrad]
	281470658252080 -> 281470658252032
	281470658029808 [label="model.encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	281470658029808 -> 281470658252080
	281470658252080 [label=AccumulateGrad]
	281470658251936 -> 281470658252032
	281470658029968 [label="model.encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	281470658029968 -> 281470658251936
	281470658251936 [label=AccumulateGrad]
	281470658251840 -> 281470658251696
	281470658030528 [label="model.encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	281470658030528 -> 281470658251840
	281470658251840 [label=AccumulateGrad]
	281470658251600 -> 281470658251504
	281470658030448 [label="model.encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	281470658030448 -> 281470658251600
	281470658251600 [label=AccumulateGrad]
	281470658251552 -> 281470658251504
	281470658030608 [label="model.encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	281470658030608 -> 281470658251552
	281470658251552 [label=AccumulateGrad]
	281470658251456 -> 281470658251408
	281470658251456 [label=NativeBatchNormBackward0]
	281470658252368 -> 281470658251456
	281470658252368 [label=ConvolutionBackward0]
	281470658247024 -> 281470658252368
	281470658252272 -> 281470658252368
	281470658029088 [label="model.encoder.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	281470658029088 -> 281470658252272
	281470658252272 [label=AccumulateGrad]
	281470658251984 -> 281470658251456
	281470658029168 [label="model.encoder.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	281470658029168 -> 281470658251984
	281470658251984 [label=AccumulateGrad]
	281470658251744 -> 281470658251456
	281470658029248 [label="model.encoder.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	281470658029248 -> 281470658251744
	281470658251744 [label=AccumulateGrad]
	281470658251312 -> 281470658251120
	281470658031088 [label="model.encoder.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	281470658031088 -> 281470658251312
	281470658251312 [label=AccumulateGrad]
	281470658251072 -> 281470658251024
	281470658031008 [label="model.encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	281470658031008 -> 281470658251072
	281470658251072 [label=AccumulateGrad]
	281470658250928 -> 281470658251024
	281470658031168 [label="model.encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	281470658031168 -> 281470658250928
	281470658250928 [label=AccumulateGrad]
	281470658250832 -> 281470658250688
	281470658031728 [label="model.encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	281470658031728 -> 281470658250832
	281470658250832 [label=AccumulateGrad]
	281470658250640 -> 281470658250544
	281470658031648 [label="model.encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	281470658031648 -> 281470658250640
	281470658250640 [label=AccumulateGrad]
	281470658250592 -> 281470658250544
	281470658031808 [label="model.encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	281470658031808 -> 281470658250592
	281470658250592 [label=AccumulateGrad]
	281470658250496 -> 281470658250448
	281470658250352 -> 281470658250160
	281470658033088 [label="model.encoder.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	281470658033088 -> 281470658250352
	281470658250352 [label=AccumulateGrad]
	281470658250112 -> 281470658250064
	281470658033008 [label="model.encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	281470658033008 -> 281470658250112
	281470658250112 [label=AccumulateGrad]
	281470658249968 -> 281470658250064
	281470658033168 [label="model.encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	281470658033168 -> 281470658249968
	281470658249968 [label=AccumulateGrad]
	281470658249872 -> 281470658249728
	281470658033728 [label="model.encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	281470658033728 -> 281470658249872
	281470658249872 [label=AccumulateGrad]
	281470658249680 -> 281470658249584
	281470658033648 [label="model.encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	281470658033648 -> 281470658249680
	281470658249680 [label=AccumulateGrad]
	281470658249632 -> 281470658249584
	281470658033808 [label="model.encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	281470658033808 -> 281470658249632
	281470658249632 [label=AccumulateGrad]
	281470658249536 -> 281470658249488
	281470658249536 [label=NativeBatchNormBackward0]
	281470658250400 -> 281470658249536
	281470658250400 [label=ConvolutionBackward0]
	281470658248128 -> 281470658250400
	281470658250304 -> 281470658250400
	281470658032288 [label="model.encoder.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	281470658032288 -> 281470658250304
	281470658250304 [label=AccumulateGrad]
	281470658250016 -> 281470658249536
	281470658032368 [label="model.encoder.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	281470658032368 -> 281470658250016
	281470658250016 [label=AccumulateGrad]
	281470658249776 -> 281470658249536
	281470658032448 [label="model.encoder.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	281470658032448 -> 281470658249776
	281470658249776 [label=AccumulateGrad]
	281470658249392 -> 281470658249200
	281470658034128 [label="model.encoder.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	281470658034128 -> 281470658249392
	281470658249392 [label=AccumulateGrad]
	281470658249152 -> 281470658249104
	281470658034208 [label="model.encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	281470658034208 -> 281470658249152
	281470658249152 [label=AccumulateGrad]
	281470658249008 -> 281470658249104
	281470658034288 [label="model.encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	281470658034288 -> 281470658249008
	281470658249008 [label=AccumulateGrad]
	281470658248912 -> 281470658248656
	281470658034848 [label="model.encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	281470658034848 -> 281470658248912
	281470658248912 [label=AccumulateGrad]
	281470658248608 -> 281470658248560
	281470658034768 [label="model.encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	281470658034768 -> 281470658248608
	281470658248608 [label=AccumulateGrad]
	281470658248768 -> 281470658248560
	281470658034928 [label="model.encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	281470658034928 -> 281470658248768
	281470658248768 [label=AccumulateGrad]
	281470658248512 -> 281470658248416
	281470658248128 -> 281470658248032
	281470658247984 -> 281470658247840
	281470657497040 [label="model.decoder.blocks.0.conv1.0.weight
 (256, 768, 3, 3)" fillcolor=lightblue]
	281470657497040 -> 281470658247984
	281470658247984 [label=AccumulateGrad]
	281470658247792 -> 281470658247744
	281470657506080 [label="model.decoder.blocks.0.conv1.1.weight
 (256)" fillcolor=lightblue]
	281470657506080 -> 281470658247792
	281470658247792 [label=AccumulateGrad]
	281470658247648 -> 281470658247744
	281470657503120 [label="model.decoder.blocks.0.conv1.1.bias
 (256)" fillcolor=lightblue]
	281470657503120 -> 281470658247648
	281470658247648 [label=AccumulateGrad]
	281470658247552 -> 281470658247408
	281470657498080 [label="model.decoder.blocks.0.conv2.0.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	281470657498080 -> 281470658247552
	281470658247552 [label=AccumulateGrad]
	281470658247360 -> 281470658247312
	281470657507040 [label="model.decoder.blocks.0.conv2.1.weight
 (256)" fillcolor=lightblue]
	281470657507040 -> 281470658247360
	281470658247360 [label=AccumulateGrad]
	281470658247120 -> 281470658247312
	281470657502720 [label="model.decoder.blocks.0.conv2.1.bias
 (256)" fillcolor=lightblue]
	281470657502720 -> 281470658247120
	281470658247120 [label=AccumulateGrad]
	281470658247024 -> 281470658246928
	281470658246880 -> 281470658246736
	281470657497280 [label="model.decoder.blocks.1.conv1.0.weight
 (128, 384, 3, 3)" fillcolor=lightblue]
	281470657497280 -> 281470658246880
	281470658246880 [label=AccumulateGrad]
	281470658246688 -> 281470658246640
	281470657511360 [label="model.decoder.blocks.1.conv1.1.weight
 (128)" fillcolor=lightblue]
	281470657511360 -> 281470658246688
	281470658246688 [label=AccumulateGrad]
	281470658246544 -> 281470658246640
	281470657499840 [label="model.decoder.blocks.1.conv1.1.bias
 (128)" fillcolor=lightblue]
	281470657499840 -> 281470658246544
	281470658246544 [label=AccumulateGrad]
	281470658246448 -> 281470658246304
	281470657506240 [label="model.decoder.blocks.1.conv2.0.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	281470657506240 -> 281470658246448
	281470658246448 [label=AccumulateGrad]
	281470658246256 -> 281470658246208
	281470657496400 [label="model.decoder.blocks.1.conv2.1.weight
 (128)" fillcolor=lightblue]
	281470657496400 -> 281470658246256
	281470658246256 [label=AccumulateGrad]
	281470658246016 -> 281470658246208
	281470657498640 [label="model.decoder.blocks.1.conv2.1.bias
 (128)" fillcolor=lightblue]
	281470657498640 -> 281470658246016
	281470658246016 [label=AccumulateGrad]
	281470658245920 -> 281470658245824
	281470658245776 -> 281470658245632
	281470657495600 [label="model.decoder.blocks.2.conv1.0.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	281470657495600 -> 281470658245776
	281470658245776 [label=AccumulateGrad]
	281470658245584 -> 281470658245536
	281470657505200 [label="model.decoder.blocks.2.conv1.1.weight
 (64)" fillcolor=lightblue]
	281470657505200 -> 281470658245584
	281470658245584 [label=AccumulateGrad]
	281470658245440 -> 281470658245536
	281470657496880 [label="model.decoder.blocks.2.conv1.1.bias
 (64)" fillcolor=lightblue]
	281470657496880 -> 281470658245440
	281470658245440 [label=AccumulateGrad]
	281470658245344 -> 281470658245200
	281470657502960 [label="model.decoder.blocks.2.conv2.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	281470657502960 -> 281470658245344
	281470658245344 [label=AccumulateGrad]
	281470658245152 -> 281470658245104
	281470657496480 [label="model.decoder.blocks.2.conv2.1.weight
 (64)" fillcolor=lightblue]
	281470657496480 -> 281470658245152
	281470658245152 [label=AccumulateGrad]
	281470658244912 -> 281470658245104
	281470657502640 [label="model.decoder.blocks.2.conv2.1.bias
 (64)" fillcolor=lightblue]
	281470657502640 -> 281470658244912
	281470658244912 [label=AccumulateGrad]
	281470658244816 -> 281470658244720
	281470658244672 -> 281470658244528
	281470657503040 [label="model.decoder.blocks.3.conv1.0.weight
 (32, 128, 3, 3)" fillcolor=lightblue]
	281470657503040 -> 281470658244672
	281470658244672 [label=AccumulateGrad]
	281470658244480 -> 281470658244432
	281470657502160 [label="model.decoder.blocks.3.conv1.1.weight
 (32)" fillcolor=lightblue]
	281470657502160 -> 281470658244480
	281470658244480 [label=AccumulateGrad]
	281470658244336 -> 281470658244432
	281470657498960 [label="model.decoder.blocks.3.conv1.1.bias
 (32)" fillcolor=lightblue]
	281470657498960 -> 281470658244336
	281470658244336 [label=AccumulateGrad]
	281470658244240 -> 281470658244096
	281470657499520 [label="model.decoder.blocks.3.conv2.0.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	281470657499520 -> 281470658244240
	281470658244240 [label=AccumulateGrad]
	281470658244048 -> 281470658244000
	281470657501120 [label="model.decoder.blocks.3.conv2.1.weight
 (32)" fillcolor=lightblue]
	281470657501120 -> 281470658244048
	281470658244048 [label=AccumulateGrad]
	281470658243808 -> 281470658244000
	281470657504320 [label="model.decoder.blocks.3.conv2.1.bias
 (32)" fillcolor=lightblue]
	281470657504320 -> 281470658243808
	281470658243808 [label=AccumulateGrad]
	281470658243712 -> 281470658243376
	281470657499920 [label="model.decoder.blocks.4.conv1.0.weight
 (16, 32, 3, 3)" fillcolor=lightblue]
	281470657499920 -> 281470658243712
	281470658243712 [label=AccumulateGrad]
	281470658243424 -> 281470658243328
	281470657497600 [label="model.decoder.blocks.4.conv1.1.weight
 (16)" fillcolor=lightblue]
	281470657497600 -> 281470658243424
	281470658243424 [label=AccumulateGrad]
	281470658242368 -> 281470658243328
	281470657509280 [label="model.decoder.blocks.4.conv1.1.bias
 (16)" fillcolor=lightblue]
	281470657509280 -> 281470658242368
	281470658242368 [label=AccumulateGrad]
	281470658243184 -> 281470658241696
	281470657495440 [label="model.decoder.blocks.4.conv2.0.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	281470657495440 -> 281470658243184
	281470658243184 [label=AccumulateGrad]
	281470658241312 -> 281470657955632
	281470657510160 [label="model.decoder.blocks.4.conv2.1.weight
 (16)" fillcolor=lightblue]
	281470657510160 -> 281470658241312
	281470658241312 [label=AccumulateGrad]
	281470658242416 -> 281470657955632
	281470657496080 [label="model.decoder.blocks.4.conv2.1.bias
 (16)" fillcolor=lightblue]
	281470657496080 -> 281470658242416
	281470658242416 [label=AccumulateGrad]
	281470658242752 -> 281470657804368
	281470657499120 [label="model.segmentation_head.0.weight
 (1, 16, 3, 3)" fillcolor=lightblue]
	281470657499120 -> 281470658242752
	281470658242752 [label=AccumulateGrad]
	281470658242848 -> 281470657804368
	281470657500560 [label="model.segmentation_head.0.bias
 (1)" fillcolor=lightblue]
	281470657500560 -> 281470658242848
	281470658242848 [label=AccumulateGrad]
	281473235041104 -> 281473235041488
	281473235041104 [label=ClampMinBackward0]
	281470657803552 -> 281473235041104
	281470657803552 [label=AddBackward0]
	281470657804752 -> 281470657803552
	281470657804752 [label=SubBackward0]
	281470657793424 -> 281470657804752
	281470657793424 [label=SumBackward1]
	281470658243280 -> 281470657793424
	281470658243280 [label=AddBackward0]
	281470657801776 -> 281470658243280
	281470657955680 -> 281470657804752
	281470657864288 -> 281470657131728
}
